{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CocoTheAussieCat/dl_at3/blob/master/Experimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK7eXHIsc6t",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive Data Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBM_yUOsezf",
        "colab_type": "code",
        "outputId": "eaa95f1f-fb65-4061-acce-b464f433914a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzzNZH5tvKB",
        "colab_type": "code",
        "outputId": "191431a6-e7b6-4dc5-c919-3a35292ba95d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Direct the workbook into the project folder\n",
        "\n",
        "%cd drive/Shared\\ drives/DL_AT3/Experimentation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Experimentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X0FuZOkWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "dataset_dir = os.getcwd() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3_tIgRkfeL",
        "colab_type": "code",
        "outputId": "b8d03e23-6458-42e3-ec40-8e0827c557ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the directory\n",
        "dataset_dir"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/Experimentation'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a1biZwZGqt",
        "colab_type": "code",
        "outputId": "1612c72f-8ab3-49e9-d4fc-1b46198744ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Experimentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGMBdWc0SVR",
        "colab_type": "code",
        "outputId": "33a6d721-6c1e-4eeb-b495-f9cbe08f91f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "#Check the files inside the directory\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints\t\t    Flicker8k_Dataset\t      Flickr8k.token.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr8k_Dataset.zip      Flickr_8k.trainImages.txt\n",
            "descriptions.txt\t    Flickr_8k.devImages.txt   __MACOSX\n",
            "Experimentation.ipynb\t    Flickr8k.lemma.token.txt  model.png\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt  readme.txt\n",
            "features_VGG16.pkl\t    Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32A5lyYpJZP",
        "colab_type": "text"
      },
      "source": [
        "## Unzip File from Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo61q99biV6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Image Dataset\n",
        "#!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUrNOy-UAGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Text Dataset\n",
        "#!unzip Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5wTgKJPjxaL",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7XZ57Sj4TC",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQY7v5tkDqA",
        "colab_type": "text"
      },
      "source": [
        "Import libraries, set working directory and relative paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzW9k4cj0oF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2608e83-a380-4cfa-f457-68847059fc30"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Embedding, Dropout, Add\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\t\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NsXujecUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD1R001PnQDN",
        "colab_type": "code",
        "outputId": "32575f1a-d03b-429c-a026-8c2968c00eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# Check the folder's content after unziping \n",
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints\t\t    Flicker8k_Dataset\t      Flickr8k.token.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr8k_Dataset.zip      Flickr_8k.trainImages.txt\n",
            "descriptions.txt\t    Flickr_8k.devImages.txt   __MACOSX\n",
            "Experimentation.ipynb\t    Flickr8k.lemma.token.txt  model.png\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt  readme.txt\n",
            "features_VGG16.pkl\t    Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV41L_nfkYWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dataset directory and relative directories\n",
        "image_dir = dataset_dir + '/Flicker8k_Dataset'\n",
        "caption_dir = dataset_dir + '/Flickr8k.token.txt'\n",
        "train_dir = dataset_dir + '/Flickr_8k.trainImages.txt'\n",
        "test_dir = dataset_dir + '/Flickr_8k.testImages.txt'\n",
        "val_dir = dataset_dir + '/Flickr_8k.devImages.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBnb6t25n7dn",
        "colab_type": "text"
      },
      "source": [
        "# Prepare image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCw-CzHVn_kC",
        "colab_type": "text"
      },
      "source": [
        "Extract image features\n",
        "\n",
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Used VGG16 pre-trained model to extract image features by:\n",
        "\n",
        "Loading VGG16 pre-trained model.\n",
        "Removing top layer (because this layer is used for classification, which is not what is required)\n",
        "Extract features from each image by using predict function of VGG16 model.\n",
        "Create image_id by extracting the characters before .jpg in the file name.\n",
        "Store these features as vector of length 4096 in dictionary with image_id as key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRubioH2AWUe",
        "colab_type": "text"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "# Use VGG16 model, without top layer, add flatten and dense layer to get output of 4096\n",
        "# which is the required shape for LSTM model\n",
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "\tmodel_new = tf.keras.Sequential([\n",
        "  \tmodel,\n",
        "  \tFlatten(),\n",
        "\tDense(4096)])\n",
        "\t# remove top layer from model\n",
        "\t# model.layers.pop()\n",
        "\t# model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# print summary\n",
        "\tprint(model_new.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict() # create empty dictionary to store features in\n",
        "\tfor name in os.listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare image for VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model_new.predict(image, verbose=0)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature in dictionary using image_id as key\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\tprint('>%s' % name)\n",
        "\treturn features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKsiIStoK3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ONLY RUN IF YOU DON'T HAVE features.pkl IN YOUR ENVIRONMENT\n",
        "### TAKES >  1HOUR TO RUN\n",
        "# Extract features from all images\n",
        "#features = extract_features(image_dir)\n",
        "#print('Extracted Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJ_UxHooeAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save feature as pickle file\n",
        "#dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrZQgR9oYpZ",
        "colab_type": "text"
      },
      "source": [
        "# Prepare text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0HSuKMtzEpp",
        "colab_type": "text"
      },
      "source": [
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Get cleaned caption for each image by:\n",
        "\n",
        "Loading captions from text file.\n",
        "Creating dictionary of captions using image_id as key.\n",
        "Clean all captions by removing digits, single letter words (eg: a), punctuation and converting to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qouDIcpC67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and read image description file\n",
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tReads all captions from txt file as single string\n",
        "\tInputs\t\t- filename = filename of .txt with image captions\n",
        "\tOutputs\t\t- text = string\n",
        "\t\"\"\"\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUlitqApC69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\t\"\"\"\n",
        "    Inputs      - doc = string, output from load_doc()\n",
        "    Outputs     - mapping = dictionary-list of image_id and captions \n",
        "    \"\"\"\n",
        "\tcaption_dict = {}\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in caption_dict:\n",
        "\t\t\tcaption_dict[image_id] = list()\n",
        "\t\t\t# store description\n",
        "\t\t\tcaption_dict [image_id].append(image_desc)\n",
        "\treturn caption_dict "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_b_6DE3pC7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsvkrzxpC7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert descriptions into vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp40MuApC7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save descriptions to file, one image_id and description per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYzk-jmpC7G",
        "colab_type": "code",
        "outputId": "d1de142e-d200-4335-804a-18bef6857131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load descriptions from tokenised text file\n",
        "doc = load_doc(caption_dir)\n",
        "\n",
        "# Create dictionary of image_id and descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Clean descriptions by stripping digits, punctuation, single letter words and converting to lowercase\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Create vocab from descriptions and get vocab length\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Save descriptions to file, one image_id and description per line\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 4473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FToIspC7J",
        "colab_type": "text"
      },
      "source": [
        "# Load pre-processed training data for modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6F1vMYpC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "\t# open file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyGLW9mZpC7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-FH_B4UpC7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMH5gzMpC7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = pickle.load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrtTPd0IpC7S",
        "colab_type": "text"
      },
      "source": [
        "## Tokenise descriptions\n",
        "Map unique words to integers using tf.keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4QHBL5opC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert dictionary of clean descriptions to list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Bh83aHpC7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0pKaW9Sn2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv8ETrBVpC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to calculate length of description with most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDMyp13LpC7f",
        "colab_type": "text"
      },
      "source": [
        "## Load train and validation data for modelling\n",
        "Images loaded as numpy arrays, descriptions tokenised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Dx0Tb0w4-O",
        "colab_type": "text"
      },
      "source": [
        "# Set training set sample size (default is set to 500 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQHAAVnIRiCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SET YOUR TRAINING SET SIZE\n",
        "train_size = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zk8KGNhAvan",
        "colab_type": "code",
        "outputId": "415108fb-97da-4d0c-cf0c-0e856b6b1b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Load training set\n",
        "train = random.sample(load_set(train_dir), train_size)\n",
        "\n",
        "# Take a random sample of 800 pictures\n",
        "#train = random.sample(train, 800)\n",
        "\n",
        "print('Dataset: %d' % len(train))\n",
        "\n",
        "# Load training set descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train = %d' % len(train_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "train_features = load_photo_features('features_VGG16.pkl', train)\n",
        "print('Photos: train = %d' % len(train_features))\n",
        "\n",
        "# Prepare sequences of descriptions for train, test and validation sets\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# Determine max sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 500\n",
            "Descriptions: train = 500\n",
            "Photos: train = 500\n",
            "Vocabulary Size: 1069\n",
            "Description Length: 26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSEqSUTEA5rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1qy26whxN3P",
        "colab_type": "text"
      },
      "source": [
        "## Set validation set sample size (default is set to 100 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQl-Udq1xT74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKQ-7sGQpC7j",
        "colab_type": "code",
        "outputId": "784acf6f-a9e0-4f80-8238-32640031508d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Load validation set (using devImages)\n",
        "val = random.sample(load_set(val_dir), val_size)\n",
        "\n",
        "print('Dataset: %d' % len(val))\n",
        "\n",
        "# Load training set descriptions\n",
        "val_descriptions = load_clean_descriptions('descriptions.txt', val)\n",
        "print('Descriptions: val = %d' % len(val_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "val_features = load_photo_features('features_VGG16.pkl', val)\n",
        "print('Photos: val = %d' % len(val_features))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 100\n",
            "Descriptions: val = 100\n",
            "Photos: val = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMvzjme4upR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1val, X2val, yval = create_sequences(tokenizer, max_length, val_descriptions, val_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt1mIMIopC7n",
        "colab_type": "text"
      },
      "source": [
        "# Define model\n",
        "Based on merge-model described by Tanti et al. in *Where to put the Image in an Image Caption Generator*\n",
        "\n",
        "source: <https://arxiv.org/abs/1703.09137>\n",
        "\n",
        "code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZllMSBpC7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = Add()([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\toptimizer = tf.keras.optimizers.Adam(0.01)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
        "\t# summary\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWe-aYjpC7q",
        "colab_type": "text"
      },
      "source": [
        "# Train model\n",
        "Use checkpoint callbacks to save training informatoin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngMLZQdpC7t",
        "colab_type": "code",
        "outputId": "3845bc1e-aa91-4fa9-e791-7154328e4c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# Create base model\n",
        "model = define_model(vocab_size, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 30, 256)      985088      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 4096)         0           input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 30, 256)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          1048832     dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 256)          525312      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 256)          0           dense_6[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 3848)         988936      dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,613,960\n",
            "Trainable params: 3,613,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi12TajEpC7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### MAKE A DIRECTORY TO SAVE CHECKPOINTS TO ###\n",
        "# eg: check_path = 'checkpoints/rebecca/exp_2'\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "check_path = 'checkpoints/rebecca/exp_2' # CHANGE THIS\n",
        "!mkdir -p $check_path\n",
        "\n",
        "#################################################################\n",
        "check_dir = check_path+'/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\n",
        "# Monitor validation loss, saving only the best, stopping early if no improvement after 5 epochs, dropping learning rate if loss plateaux\n",
        "checkpoint = ModelCheckpoint(check_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfje5knBVYR",
        "colab_type": "code",
        "outputId": "11d82d3b-15e6-46bf-b2fe-ed9ead23214a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "source": [
        "# Fit model\n",
        "tic = time.perf_counter()\n",
        "history = model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, \n",
        "                    validation_data=([X1val, X2val], yval), \n",
        "                    callbacks = [checkpoint, early_stopping, reduce_lr],\n",
        "                    )\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.55120, saving model to checkpoints/rebecca/exp_2/model-ep001-loss6.098-val_loss5.551.h5\n",
            "1916/1916 - 399s - loss: 6.0981 - accuracy: 0.0969 - val_loss: 5.5512 - val_accuracy: 0.0991 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.55120 to 5.53920, saving model to checkpoints/rebecca/exp_2/model-ep002-loss5.700-val_loss5.539.h5\n",
            "1916/1916 - 393s - loss: 5.7004 - accuracy: 0.0979 - val_loss: 5.5392 - val_accuracy: 0.0991 - lr: 0.0100\n",
            "Epoch 3/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-a53eb63cdc8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m history = model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, \n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                     )\n\u001b[1;32m      7\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tobtVqSdm7E",
        "colab_type": "text"
      },
      "source": [
        "## Plot accuracy and loss of training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-yAFWoHcpF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelPlot(history_name, plt_title):\n",
        "    \"\"\"\n",
        "    Generates plot (with title) for model training and validation accuracy and loss by epoch\n",
        "    Inputs  - history_name = history object resulting from fitted model\n",
        "            - plt_title = title for plot, as string\n",
        "\n",
        "    Output  - plot\n",
        "    \"\"\"\n",
        "\n",
        "    accuracy_filename = check_dir+'/acc.png'\n",
        "    loss_filename = check_dir+'/loss.png'\n",
        "\n",
        "    plt.plot(trained_model.history['accuracy'], label='Training', color = 'purple')\n",
        "    plt.plot(trained_model.history['val_accuracy'], label='Validation', color = 'orange')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title(plt_title)\n",
        "    plt.legend()\n",
        "    plt.savefig(accuracy_filename)\n",
        "\n",
        "    plt.plot(history_name.history['loss'], label='Training', color = 'blue')\n",
        "    plt.plot(history_name.history['val_loss'], label='Validation', color = 'orange')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title(plt_title)\n",
        "    plt.legend()\n",
        "    plt.savefig(loss_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HxSt93IcugA",
        "colab_type": "code",
        "outputId": "3fcf07c4-9a72-440b-9550-60650ac5037a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "modelPlot(history, 'Experiment 1')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEYDMRJldwVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import savefig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68lMBug1fcia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCEX3ethL49",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qk3hHaDhOGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse vectorisation of words to map integers back to words\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVv_Kz5hejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over whole length of sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlytc6SphqTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8hiqXV_kyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### FIND YOUR BEST MODEL FROM THE MODEL.FIT OUTPUT AND COPY THE NAME BELOW #######\n",
        "# Get best model filename from model.fit output\n",
        "checkpoint_best_model = 'checkpoints/rebecca/exp_1/model-ep014-loss4.074-val_loss4.448.h5' # CHANGE THIS\n",
        "\n",
        "# Load best model\n",
        "best_model = tf.keras.models.load_model(checkpoint_best_model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp3Q1MFzlB3B",
        "colab_type": "code",
        "outputId": "d48df6d9-cb53-4406-f288-98b8d67f13e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "tic = time.perf_counter()\n",
        "evaluate_model(best_model, val_descriptions, val_features, tokenizer, max_length)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.4f} minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.312347\n",
            "BLEU-2: 0.104778\n",
            "BLEU-3: 0.061958\n",
            "BLEU-4: 0.014870\n",
            "Model ran in: 6.4194 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}