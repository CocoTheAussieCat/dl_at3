{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CocoTheAussieCat/dl_at3/blob/Rob/Experimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK7eXHIsc6t",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive Data Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBM_yUOsezf",
        "colab_type": "code",
        "outputId": "b9969996-e0d3-4173-f56f-7c52f97866d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzzNZH5tvKB",
        "colab_type": "code",
        "outputId": "dd7671b9-d64b-4083-82d2-34626e7eb9be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Direct the workbook into the project folder\n",
        "\n",
        "%cd drive/Shared\\ drives/DL_AT3/Experimentation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Experimentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X0FuZOkWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "dataset_dir = os.getcwd() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3_tIgRkfeL",
        "colab_type": "code",
        "outputId": "0a565c09-d3ae-4d62-f769-33b8654d19f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the directory\n",
        "dataset_dir"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/Experimentation'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a1biZwZGqt",
        "colab_type": "code",
        "outputId": "195fd057-a291-401a-82dd-29e88cc6d4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Experimentation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGMBdWc0SVR",
        "colab_type": "code",
        "outputId": "b9e0cf65-2857-4730-89e4-7f3a2aca0a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#Check the files inside the directory\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "archive\t\t\t    features_VGG16.pkl\t      Flickr8k.token.txt\n",
            "checkpoints\t\t    Flicker8k_Dataset\t      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr8k_Dataset.zip      __MACOSX\n",
            "descriptions.txt\t    Flickr_8k.devImages.txt   model.png\n",
            "Experimentation.ipynb\t    Flickr8k.lemma.token.txt  readme.txt\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32A5lyYpJZP",
        "colab_type": "text"
      },
      "source": [
        "## Unzip File from Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo61q99biV6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Image Dataset\n",
        "#!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUrNOy-UAGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Text Dataset\n",
        "#!unzip Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5wTgKJPjxaL",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7XZ57Sj4TC",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQY7v5tkDqA",
        "colab_type": "text"
      },
      "source": [
        "Import libraries, set working directory and relative paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzW9k4cj0oF",
        "colab_type": "code",
        "outputId": "7fa351f9-17f3-4ed4-c6bf-cb4d7965df77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Embedding, Dropout, Add\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\t\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NsXujecUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD1R001PnQDN",
        "colab_type": "code",
        "outputId": "4f3a3262-c731-4fdc-b7d8-fe5a943150e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "# Check the folder's content after unziping \n",
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "archive\t\t\t    features_VGG16.pkl\t      Flickr8k.token.txt\n",
            "checkpoints\t\t    Flicker8k_Dataset\t      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr8k_Dataset.zip      __MACOSX\n",
            "descriptions.txt\t    Flickr_8k.devImages.txt   model.png\n",
            "Experimentation.ipynb\t    Flickr8k.lemma.token.txt  readme.txt\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV41L_nfkYWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dataset directory and relative directories\n",
        "image_dir = dataset_dir + '/Flicker8k_Dataset'\n",
        "caption_dir = dataset_dir + '/Flickr8k.token.txt'\n",
        "train_dir = dataset_dir + '/Flickr_8k.trainImages.txt'\n",
        "test_dir = dataset_dir + '/Flickr_8k.testImages.txt'\n",
        "val_dir = dataset_dir + '/Flickr_8k.devImages.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBnb6t25n7dn",
        "colab_type": "text"
      },
      "source": [
        "# Prepare image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCw-CzHVn_kC",
        "colab_type": "text"
      },
      "source": [
        "Extract image features\n",
        "\n",
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Used VGG16 pre-trained model to extract image features by:\n",
        "\n",
        "Loading VGG16 pre-trained model.\n",
        "Removing top layer (because this layer is used for classification, which is not what is required)\n",
        "Extract features from each image by using predict function of VGG16 model.\n",
        "Create image_id by extracting the characters before .jpg in the file name.\n",
        "Store these features as vector of length 4096 in dictionary with image_id as key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRubioH2AWUe",
        "colab_type": "text"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "# Use VGG16 model, without top layer, add flatten and dense layer to get output of 4096\n",
        "# which is the required shape for LSTM model\n",
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "\tmodel_new = tf.keras.Sequential([\n",
        "  \tmodel,\n",
        "  \tFlatten(),\n",
        "\tDense(4096)])\n",
        "\t# remove top layer from model\n",
        "\t# model.layers.pop()\n",
        "\t# model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# print summary\n",
        "\tprint(model_new.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict() # create empty dictionary to store features in\n",
        "\tfor name in os.listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare image for VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model_new.predict(image, verbose=0)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature in dictionary using image_id as key\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\tprint('>%s' % name)\n",
        "\treturn features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKsiIStoK3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ONLY RUN IF YOU DON'T HAVE features.pkl IN YOUR ENVIRONMENT\n",
        "### TAKES >  1HOUR TO RUN\n",
        "# Extract features from all images\n",
        "#features = extract_features(image_dir)\n",
        "#print('Extracted Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJ_UxHooeAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save feature as pickle file\n",
        "#dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrZQgR9oYpZ",
        "colab_type": "text"
      },
      "source": [
        "# Prepare text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0HSuKMtzEpp",
        "colab_type": "text"
      },
      "source": [
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Get cleaned caption for each image by:\n",
        "\n",
        "Loading captions from text file.\n",
        "Creating dictionary of captions using image_id as key.\n",
        "Clean all captions by removing digits, single letter words (eg: a), punctuation and converting to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qouDIcpC67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and read image description file\n",
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tReads all captions from txt file as single string\n",
        "\tInputs\t\t- filename = filename of .txt with image captions\n",
        "\tOutputs\t\t- text = string\n",
        "\t\"\"\"\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUlitqApC69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\t\"\"\"\n",
        "    Inputs      - doc = string, output from load_doc()\n",
        "    Outputs     - mapping = dictionary-list of image_id and captions \n",
        "    \"\"\"\n",
        "\tcaption_dict = {}\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in caption_dict:\n",
        "\t\t\tcaption_dict[image_id] = list()\n",
        "\t\t\t# store description\n",
        "\t\t\tcaption_dict [image_id].append(image_desc)\n",
        "\treturn caption_dict "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_b_6DE3pC7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsvkrzxpC7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert descriptions into vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp40MuApC7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save descriptions to file, one image_id and description per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYzk-jmpC7G",
        "colab_type": "code",
        "outputId": "b6d96a0a-efd0-48f1-b6bf-be35668db766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load descriptions from tokenised text file\n",
        "doc = load_doc(caption_dir)\n",
        "\n",
        "# Create dictionary of image_id and descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Clean descriptions by stripping digits, punctuation, single letter words and converting to lowercase\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Create vocab from descriptions and get vocab length\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Save descriptions to file, one image_id and description per line\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 4473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FToIspC7J",
        "colab_type": "text"
      },
      "source": [
        "# Load pre-processed training data for modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6F1vMYpC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "\t# open file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyGLW9mZpC7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-FH_B4UpC7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMH5gzMpC7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = pickle.load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrtTPd0IpC7S",
        "colab_type": "text"
      },
      "source": [
        "## Tokenise descriptions\n",
        "Map unique words to integers using tf.keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4QHBL5opC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert dictionary of clean descriptions to list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Bh83aHpC7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0pKaW9Sn2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv8ETrBVpC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to calculate length of description with most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDMyp13LpC7f",
        "colab_type": "text"
      },
      "source": [
        "## Load train and validation data for modelling\n",
        "Images loaded as numpy arrays, descriptions tokenised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Dx0Tb0w4-O",
        "colab_type": "text"
      },
      "source": [
        "# Set training set sample size (default is set to 500 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQHAAVnIRiCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SET YOUR TRAINING SET SIZE\n",
        "train_size = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zk8KGNhAvan",
        "colab_type": "code",
        "outputId": "cbd7e44d-99ea-41fb-e577-056e8fda3a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Load training set\n",
        "train = random.sample(load_set(train_dir), train_size)\n",
        "\n",
        "# Take a random sample of 800 pictures\n",
        "#train = random.sample(train, 800)\n",
        "\n",
        "print('Dataset: %d' % len(train))\n",
        "\n",
        "# Load training set descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train = %d' % len(train_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "train_features = load_photo_features('features_VGG16.pkl', train)\n",
        "print('Photos: train = %d' % len(train_features))\n",
        "\n",
        "# Prepare sequences of descriptions for train, test and validation sets\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# Determine max sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 500\n",
            "Descriptions: train = 500\n",
            "Photos: train = 500\n",
            "Vocabulary Size: 1022\n",
            "Description Length: 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSEqSUTEA5rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1qy26whxN3P",
        "colab_type": "text"
      },
      "source": [
        "## Set validation set sample size (default is set to 100 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQl-Udq1xT74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKQ-7sGQpC7j",
        "colab_type": "code",
        "outputId": "2ca7c602-0299-403c-e9e2-38290330a388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Load validation set (using devImages)\n",
        "val = random.sample(load_set(val_dir), val_size)\n",
        "\n",
        "print('Dataset: %d' % len(val))\n",
        "\n",
        "# Load training set descriptions\n",
        "val_descriptions = load_clean_descriptions('descriptions.txt', val)\n",
        "print('Descriptions: val = %d' % len(val_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "val_features = load_photo_features('features_VGG16.pkl', val)\n",
        "print('Photos: val = %d' % len(val_features))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 100\n",
            "Descriptions: val = 100\n",
            "Photos: val = 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMvzjme4upR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1val, X2val, yval = create_sequences(tokenizer, max_length, val_descriptions, val_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt1mIMIopC7n",
        "colab_type": "text"
      },
      "source": [
        "# Define model\n",
        "Based on merge-model described by Tanti et al. in *Where to put the Image in an Image Caption Generator*\n",
        "\n",
        "source: <https://arxiv.org/abs/1703.09137>\n",
        "\n",
        "code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZllMSBpC7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = Add()([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\toptimizer = tf.keras.optimizers.Adam(0.01)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
        "\t# summary\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWe-aYjpC7q",
        "colab_type": "text"
      },
      "source": [
        "# Train model\n",
        "Use checkpoint callbacks to save training informatoin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngMLZQdpC7t",
        "colab_type": "code",
        "outputId": "8b828648-c7b9-4ea0-e363-f91a091117ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "# Create base model\n",
        "model = define_model(vocab_size, max_length)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 28)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 28, 256)      261632      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 28, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1022)         262654      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,164,222\n",
            "Trainable params: 2,164,222\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi12TajEpC7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### MAKE A DIRECTORY TO SAVE CHECKPOINTS TO ###\n",
        "# eg: check_path = 'checkpoints/rebecca/exp_2'\n",
        "###########################################################################\n",
        "\n",
        "\n",
        "check_path = 'checkpoints/rob/exp_1' # CHANGE THIS\n",
        "!mkdir -p $check_path\n",
        "\n",
        "#################################################################\n",
        "#check_dir = check_path+'/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "check_dir = check_path+'/exp_1.h5'\n",
        "\n",
        "# Monitor validation loss, saving only the best, stopping early if no improvement after 5 epochs, dropping learning rate if loss plateaux\n",
        "checkpoint = ModelCheckpoint(check_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwVP5cQwBTjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "daeada72-ada7-44be-a611-d1c7eeb8be9c"
      },
      "source": [
        "!"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "archive\t\t\t    features_VGG16.pkl\t      Flickr8k.token.txt\n",
            "checkpoints\t\t    Flicker8k_Dataset\t      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr8k_Dataset.zip      __MACOSX\n",
            "descriptions.txt\t    Flickr_8k.devImages.txt   model.png\n",
            "Experimentation.ipynb\t    Flickr8k.lemma.token.txt  readme.txt\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfje5knBVYR",
        "colab_type": "code",
        "outputId": "b14ad4b7-8fb1-4c5e-ab6f-e808e45a9407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit model\n",
        "tic = time.perf_counter()\n",
        "history = model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, \n",
        "                    validation_data=([X1val, X2val], yval), \n",
        "                    callbacks = [checkpoint, early_stopping, reduce_lr],\n",
        "                    )\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.11112, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 8.2701 - accuracy: 0.0304 - val_loss: 6.1111 - val_accuracy: 0.0419 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.11112 to 5.61046, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.9990 - accuracy: 0.0932 - val_loss: 5.6105 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.61046 to 5.33177, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.6669 - accuracy: 0.0992 - val_loss: 5.3318 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.33177 to 5.22037, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.5165 - accuracy: 0.0992 - val_loss: 5.2204 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 5.22037 to 5.17725, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4678 - accuracy: 0.0992 - val_loss: 5.1773 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 5.17725 to 5.15794, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4524 - accuracy: 0.0992 - val_loss: 5.1579 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 5.15794 to 5.14772, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4457 - accuracy: 0.0992 - val_loss: 5.1477 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 5.14772 to 5.14089, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4422 - accuracy: 0.0992 - val_loss: 5.1409 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 5.14089 to 5.13720, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4493 - accuracy: 0.0992 - val_loss: 5.1372 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 5.13720 to 5.13204, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4995 - accuracy: 0.0990 - val_loss: 5.1320 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 5.13204 to 5.12940, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4384 - accuracy: 0.0992 - val_loss: 5.1294 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 5.12940 to 5.12818, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4380 - accuracy: 0.0992 - val_loss: 5.1282 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 5.12818 to 5.12605, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4376 - accuracy: 0.0992 - val_loss: 5.1260 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 5.12605 to 5.12586, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4374 - accuracy: 0.0992 - val_loss: 5.1259 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 5.12586 to 5.12545, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4374 - accuracy: 0.0992 - val_loss: 5.1255 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss improved from 5.12545 to 5.12275, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 26s - loss: 5.4373 - accuracy: 0.0992 - val_loss: 5.1228 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.12275\n",
            "158/158 - 26s - loss: 5.4374 - accuracy: 0.0992 - val_loss: 5.1243 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss improved from 5.12275 to 5.12222, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4372 - accuracy: 0.0992 - val_loss: 5.1222 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss improved from 5.12222 to 5.12171, saving model to checkpoints/rob/exp_1/exp_1.h5\n",
            "158/158 - 27s - loss: 5.4371 - accuracy: 0.0992 - val_loss: 5.1217 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.12171\n",
            "158/158 - 26s - loss: 5.4372 - accuracy: 0.0992 - val_loss: 5.1218 - val_accuracy: 0.1074 - lr: 0.0100\n",
            "Model ran in: 8.88 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tobtVqSdm7E",
        "colab_type": "text"
      },
      "source": [
        "## Plot accuracy and loss of training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo-KIkN0Er42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-yAFWoHcpF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelPlot(history_name, plt_title):\n",
        "    \"\"\"\n",
        "    Generates plot (with title) for model training and validation accuracy and loss by epoch\n",
        "    Inputs  - history_name = history object resulting from fitted model\n",
        "            - plt_title = title for plot, as string\n",
        "    Output  - plot\n",
        "    \"\"\"\n",
        "    from matplotlib.pyplot import savefig\n",
        "    accuracy_filename = check_path+'/acc.png'\n",
        "    loss_filename = check_path+'/loss.png'\n",
        "    plt.plot(history_name.history['accuracy'], label='Training', color = 'blue')\n",
        "    plt.plot(history_name.history['val_accuracy'], label='Validation', color = 'orange')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title(plt_title)\n",
        "    plt.legend()\n",
        "    plt.savefig(accuracy_filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    plt.plot(history_name.history['loss'], label='Training', color = 'blue')\n",
        "    plt.plot(history_name.history['val_loss'], label='Validation', color = 'orange')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title(plt_title)\n",
        "    plt.legend()\n",
        "    plt.savefig(loss_filename)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HxSt93IcugA",
        "colab_type": "code",
        "outputId": "1af1f53f-200d-4f75-f62d-e1cd774b5397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "modelPlot(history, 'Experiment 1')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEYDMRJldwVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import savefig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCEX3ethL49",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qk3hHaDhOGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse vectorisation of words to map integers back to words\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVv_Kz5hejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over whole length of sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlytc6SphqTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8hiqXV_kyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### FIND YOUR BEST MODEL FROM THE MODEL.FIT OUTPUT AND COPY THE NAME BELOW #######\n",
        "# Get best model filename from model.fit output\n",
        "checkpoint_best_model = 'checkpoints/rob/exp_1/exp_1.h5' # CHANGE THIS\n",
        "\n",
        "# Load best model\n",
        "best_model = tf.keras.models.load_model(checkpoint_best_model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp3Q1MFzlB3B",
        "colab_type": "code",
        "outputId": "6c984cd0-b4a5-4f45-fc0d-67a8fae25d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "tic = time.perf_counter()\n",
        "evaluate_model(best_model, val_descriptions, val_features, tokenizer, max_length)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.4f} minutes')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.007119\n",
            "BLEU-2: 0.007119\n",
            "BLEU-3: 0.007119\n",
            "BLEU-4: 0.007119\n",
            "Model ran in: 0.0729 minutes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}