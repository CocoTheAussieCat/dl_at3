{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Model_for_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CocoTheAussieCat/dl_at3/blob/master/Final_Model_for_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJX7OBOkLSfB",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning - AT3 - Final Model\n",
        "Team name: **AROCworkOrange**\n",
        "\n",
        "Please run final cell for validation of results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYOrTEFvKmg6",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "This notebook is based on code from Akshat Maheshwari's *Image Captioning using Keras (in Python)*, n.d.,  <https://iq.opengenus.org/image-captioning-using-keras/> and the associated GitHub repository *Image-Captioning-Using-Keras*, <https://github.com/akki3d76/Image-Captioning-Using-Keras>.\n",
        "\n",
        "The code for data pre-processing, model architecture and evaluation with BLEU scores has been adapted from Maheshwari's original work. \n",
        "\n",
        "The data generator code has been adapted from Jeff Heaton's original work in the GitHub repository *T81-588 Applications of Deep Neural Networks*, <https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_4_captioning.ipynb>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK7eXHIsc6t",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive -- RUN\n",
        "All data is located in `Experimentation` folder of Google Drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXFt04pcxuP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use most recent version of tensorflow\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBM_yUOsezf",
        "colab_type": "code",
        "outputId": "94194b3e-f831-41f2-8ad8-6d43d18662dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZUvUsELpa1",
        "colab_type": "text"
      },
      "source": [
        "## Set directory\n",
        "This should be the base directory containing all the Flickr8k files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzzNZH5tvKB",
        "colab_type": "code",
        "outputId": "5b428518-3602-46f6-dce2-e8a946d8110b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Direct the workbook into the project folder\n",
        "%cd drive/Shared\\ drives/DL_AT3/ImageCaptioning"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/ImageCaptioning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X0FuZOkWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "dataset_dir = os.getcwd() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3_tIgRkfeL",
        "colab_type": "code",
        "outputId": "551ed35d-84e7-415b-fd81-daf0ee2e6e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check the directory\n",
        "dataset_dir"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/ImageCaptioning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a1biZwZGqt",
        "colab_type": "code",
        "outputId": "a8bf8e00-dc87-4ee1-a733-afc6735f8f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check working director set correctly\n",
        "!pwd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/ImageCaptioning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGMBdWc0SVR",
        "colab_type": "code",
        "outputId": "d9525b21-47ab-4b57-a8db-607e68f75a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Check the files inside the directory\n",
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "captions.txt\t\t    Flickr8k.token.txt\n",
            "checkpoints\t\t    Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  glove.6B.200d.txt\n",
            "ExpertAnnotations.txt\t    Group_model.ipynb\n",
            "Flicker8k_Dataset\t    __MACOSX\n",
            "Flickr8k_Dataset.zip\t    readme.txt\n",
            "Flickr_8k.devImages.txt     test_features_inception.pkl\n",
            "Flickr8k.lemma.token.txt    train_features_inception.pkl\n",
            "Flickr_8k.testImages.txt    val_features_inception.pkl\n",
            "Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32A5lyYpJZP",
        "colab_type": "text"
      },
      "source": [
        "## Unzip files\n",
        "Only required once, if files are already lists in the directory unzipping is not required again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo61q99biV6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Image Dataset\n",
        "#!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUrNOy-UAGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Text Dataset\n",
        "#!unzip Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83HvfBCS9Rm_",
        "colab_type": "text"
      },
      "source": [
        "## Set directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV41L_nfkYWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dataset directory and relative directories\n",
        "image_dir = dataset_dir + '/Flicker8k_Dataset'\n",
        "caption_dir = dataset_dir + '/Flickr8k.token.txt'\n",
        "train_dir = dataset_dir + '/Flickr_8k.trainImages.txt'\n",
        "test_dir = dataset_dir + '/Flickr_8k.testImages.txt'\n",
        "val_dir = dataset_dir +'/Flickr_8k.devImages.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7XZ57Sj4TC",
        "colab_type": "text"
      },
      "source": [
        "# Setup -- RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQY7v5tkDqA",
        "colab_type": "text"
      },
      "source": [
        "Import libraries, set working directory and relative paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba7KqSQxL8Jc",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzW9k4cj0oF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "from pickle import dump, load\n",
        "import string\n",
        "import os\n",
        "from time import time\n",
        "import time\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Embedding, Dropout, Add, TimeDistributed, RepeatVector, Activation, Reshape, concatenate,BatchNormalization\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\t\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DadsgAW9JDm",
        "colab_type": "text"
      },
      "source": [
        "## Set seeds\n",
        "For reproducible results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NsXujecUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dAZSt8K_bmy",
        "colab_type": "text"
      },
      "source": [
        "## Train, validation and test file names\n",
        "Create lists of dataset file names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_9hOHfTVG6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gets all files in directory and extracts filename without extension\n",
        "# Returns a list\n",
        "def img_id(filename):\n",
        "    \"\"\"\n",
        "    Inputs      - filename = directory where files are located\n",
        "    Outputs     - img_name = list of image filenames\n",
        "    \"\"\"\n",
        "    with open(filename) as file:\n",
        "        data = file.readlines()\n",
        "        img_name = []\n",
        "        for img_id in data:\n",
        "            img_name.append(img_id.split('.')[0])\n",
        "    return img_name    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEOv0jWvVTNN",
        "colab_type": "code",
        "outputId": "5dcd0473-1ed9-41d7-def3-4c09e41a3c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Create lists of filenames for train, val and test\n",
        "train_img_name = img_id(train_dir) \n",
        "val_img_name = img_id(val_dir) \n",
        "test_img_name = img_id(test_dir) \n",
        "\n",
        "# Print 5 sample image names to check function worked correctly\n",
        "print('Samples from train:')\n",
        "print(*train_img_name[:5],sep='\\n') \n",
        "\n",
        "# Check length of datasets\n",
        "print('\\nNumber of images in train data:',len(train_img_name)) \n",
        "print('\\nNumber of images in val data:',len(val_img_name))\n",
        "print('\\nNumber of images in test data:',len(test_img_name))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples from train:\n",
            "2513260012_03d33305cf\n",
            "2903617548_d3e38d7f88\n",
            "3338291921_fe7ae0c8f8\n",
            "488416045_1c6d903fe0\n",
            "2644326817_8f45080b87\n",
            "\n",
            "Number of images in train data: 6000\n",
            "\n",
            "Number of images in val data: 1000\n",
            "\n",
            "Number of images in test data: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic6Xn_TCRpku",
        "colab_type": "text"
      },
      "source": [
        "# Caption pre-processing -- DO NOT RUN\n",
        "Process caption text for input into image captioning model. \n",
        "\n",
        "Source: <https://github.com/akki3d76/Image-Captioning-Using-Keras/blob/master/Captionate.ipynb>\n",
        "\n",
        "Each image has 5 captions (#0 to #4).\n",
        "* 5 captions turned into a list with associated image id. \n",
        "\n",
        "* Captions then cleaned to remove punctuation. \n",
        "\n",
        "* Output is saved as `caption.txt` so this code DOES NOT need to be run again. The file can simply be reloaded. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inWnLLx59_Q-",
        "colab_type": "text"
      },
      "source": [
        "## Load captions from original .txt file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTQ4n5cINyFB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHxXjalxRoF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "  \"\"\"\n",
        "  Inputs      - filename = directory where captions are located\n",
        "  Outputs     - text = file read line by line\n",
        "  \"\"\"\n",
        "    with open(filename) as file:\n",
        "        text = file.readlines()\n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4CLCK39Sape",
        "colab_type": "code",
        "outputId": "2a7a0dbf-8e98-4c7d-8cce-75993ffa2ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Read in caption text file\n",
        "text = load_doc(caption_dir)\n",
        "\n",
        "# Check file has been read in correctly by looking at 10 lines\n",
        "for line in text[:10]:\n",
        "    print(line,end='')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg#3\tA little girl climbing the stairs to her playhouse .\n",
            "1000268201_693b08cb0e.jpg#4\tA little girl in a pink dress going into a wooden cabin .\n",
            "1001773457_577c3a7d70.jpg#0\tA black dog and a spotted dog are fighting\n",
            "1001773457_577c3a7d70.jpg#1\tA black dog and a tri-colored dog playing with each other on the road .\n",
            "1001773457_577c3a7d70.jpg#2\tA black dog and a white dog with brown spots are staring at each other in the street .\n",
            "1001773457_577c3a7d70.jpg#3\tTwo dogs of different breeds looking at each other on the road .\n",
            "1001773457_577c3a7d70.jpg#4\tTwo dogs on pavement moving toward each other .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJE7oV0uSJu8",
        "colab_type": "text"
      },
      "source": [
        "## Map images file names to captions\n",
        "Create a dictionary of captions with corresponding image file name (with .jpg extension removed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHN45CiaSJX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_to_captions(text):\n",
        "    \"\"\"\n",
        "    Inputs      - text = output from load_doc()\n",
        "    Outputs     - hash_map = dictionary with image id as key, captions as values\n",
        "    \"\"\"\n",
        "    hash_map = {}\n",
        "    for line in text:\n",
        "        token = line.split()\n",
        "        image_id = token[0].split('.')[0] # separating with '.' to extract image id (removing .jpg)\n",
        "        image_caption = ' '.join(token[1: ])\n",
        "        \n",
        "        if(image_id not in hash_map):\n",
        "            hash_map[image_id] = [image_caption]\n",
        "        else:\n",
        "            hash_map[image_id].append(image_caption)\n",
        "        \n",
        "    return hash_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdyP-M4MSW1Q",
        "colab_type": "code",
        "outputId": "e7fe6a76-39cb-4230-f065-d12048acabc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Create dictionary of image ids and captions\n",
        "map_img_to_captions = image_to_captions(text)\n",
        "\n",
        "# Check by printing five captions for a sample image\n",
        "print(*map_img_to_captions['1000268201_693b08cb0e'],sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "A girl going into a wooden building .\n",
            "A little girl climbing into a wooden playhouse .\n",
            "A little girl climbing the stairs to her playhouse .\n",
            "A little girl in a pink dress going into a wooden cabin .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY1880DjSycG",
        "colab_type": "text"
      },
      "source": [
        "## Clean captions\n",
        "Captions are already quite clean, with no accented characters, digits or special characters. However, punctuation needs to be stripped and all characters converted to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIgMiOxIS9es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each image, clean captions and put all five captions into single list\n",
        "def preprocess(map_img_to_captions):\n",
        "    \"\"\"\n",
        "    Inputs      - map_img_to_captions = output from image_to_captions()\n",
        "    Outputs     - map_img_to_captions = with cleaned captions\n",
        "    \"\"\"\n",
        "    preprocessed_captions = []\n",
        "    for key in map_img_to_captions.keys():\n",
        "        for idx in range(len(map_img_to_captions[key])):\n",
        "            tokens = map_img_to_captions[key][idx].split()\n",
        "            tokens = [token.lower() for token in tokens if len(token)>1 if token.isalpha()]\n",
        "            map_img_to_captions[key][idx] = ' '.join(tokens)\n",
        "            \n",
        "    return map_img_to_captions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tzELOIRS_1j",
        "colab_type": "code",
        "outputId": "b3b74371-9e71-4af2-86ac-10fffb0b466e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Clean captions\n",
        "preprocessed_map = preprocess(map_img_to_captions)\n",
        "\n",
        "# Check by printing list of captions for sample image\n",
        "preprocessed_map['1000268201_693b08cb0e']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['child in pink dress is climbing up set of stairs in an entry way',\n",
              " 'girl going into wooden building',\n",
              " 'little girl climbing into wooden playhouse',\n",
              " 'little girl climbing the stairs to her playhouse',\n",
              " 'little girl in pink dress going into wooden cabin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck9DZ3ECTWdN",
        "colab_type": "text"
      },
      "source": [
        "## Create vocabulary from captions\n",
        "Build a set with unique words from all the captions. Check by printing out the size of the vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ9n39xsTWLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a set of unique words in all the captions\n",
        "def create_vocabulary(preprocessed_map):\n",
        "    \"\"\"\n",
        "    Inputs      - preprocessed_map = output from preprocess()\n",
        "    Outputs     - vocabulary = set with all unique words from captions\n",
        "    \"\"\"\n",
        "    vocabulary = set()\n",
        "    for img_captions in preprocessed_map.values(): # list of 5 captions for each image\n",
        "        for caption in img_captions:\n",
        "            for token in caption.split():\n",
        "                vocabulary.add(token)    \n",
        "    return vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qAuQp0eTyl1",
        "colab_type": "code",
        "outputId": "6d882fc9-c1f8-476d-e647-0645a10c9cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create vocabulary set\n",
        "vocabulary = create_vocabulary(preprocessed_map)\n",
        "\n",
        "# Check vocab size\n",
        "print('Vocabulary size:',len(vocabulary))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 8357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1hoGyCHUbU5",
        "colab_type": "text"
      },
      "source": [
        "## Save captions.txt\n",
        "Save image id and clean captions as `captions.txt` in working directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsKGaRxjULdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of image ids and preprocessed captions\n",
        "# Write to txt file\n",
        "def save_captions(preprocessed_map, filename):\n",
        "    \"\"\"\n",
        "    Inputs      - preprocessed_map = output from preprocess()\n",
        "                - filename = string with .txt extension\n",
        "    Outputs     - none, file written to working directory\n",
        "    \"\"\" \n",
        "    data = []\n",
        "    for image_id,image_captions in preprocessed_map.items():\n",
        "        for caption in image_captions:\n",
        "            data.append(image_id + ' ' + caption + '\\n')\n",
        "            \n",
        "    with open(filename,'w') as file:\n",
        "        for line in data:\n",
        "            file.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTbwy8T3UhZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save captions to file in working directory\n",
        "save_captions(preprocessed_map,'captions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqhSZ55oVHmz",
        "colab_type": "text"
      },
      "source": [
        "# Image pre-processing -- DO NOT RUN\n",
        "Images are pre-processed so they can be run through Inceptions-V3 to extract features. \n",
        "\n",
        "Feature vectors saved as:\n",
        "\n",
        "* `features_train_inception.pkl`\n",
        "\n",
        "* `features_val_inception.pkl`\n",
        "\n",
        "* `features_test_inception.pkl`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReuaCTFUXD1o",
        "colab_type": "text"
      },
      "source": [
        "## Pre-process images\n",
        "Pre-process so images are in correct format to be fed into Inception-V3 model for feature extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgTrrA6VZ2ST",
        "colab_type": "text"
      },
      "source": [
        "### Create lists of train and validation image file names,including .jpg extension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5Ii4cnrZbHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def images_name(path):\n",
        "    \"\"\"\n",
        "    Inputs      - path = directory\n",
        "    Outputs     - img_name = set of unique image names\n",
        "    \"\"\"\n",
        "    img_name = set([path+image for image in os.listdir(path)])\n",
        "    return img_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCLX_A8VZnpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of image filename with .jpg extension\n",
        "all_images_filnames = images_name(image_dir)\n",
        "train_img_filenames = [image_dir + '/' + img + '.jpg' for img in train_img_name]\n",
        "val_img_filenames = [image_dir + '/' + img + '.jpg' for img in val_img_name]\n",
        "test_img_filenames = [image_dir + '/' + img + '.jpg' for img in test_img_name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZxLzsp5BJxE",
        "colab_type": "text"
      },
      "source": [
        "### Pre-process images as required for Inception-V3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tio6hMX5XM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads image, adds 4th dimension for colour channel, pre-process for Inception-V3\n",
        "# Returns array with dims (1, 299, 299, 3)\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    \"\"\"\n",
        "    Inputs      - img_path = directory\n",
        "    Outputs     - img = pre-processed image of shape (1, 299, 299, 3)\n",
        "    \"\"\"    \n",
        "    img = image.load_img(img_path,target_size=(299,299)) \n",
        "\n",
        "    # Converts PIL image to numpy array (299,299,3)\n",
        "    img = image.img_to_array(img) \n",
        "\n",
        "    # Add one more dimension: (1, 299, 299, 3)\n",
        "    img = np.expand_dims(img, axis=0) \n",
        "\n",
        "    # Pre-process image for input into Inception-V3 model\n",
        "    img = preprocess_input(img) \n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "639bGqA8X08h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check preprocess_image works by running one sample image through it\n",
        "\n",
        "sample_image = image_dir +'/1000268201_693b08cb0e.jpg'\n",
        "img = preprocess_image(sample_image)\n",
        "plt.imshow(img[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3KYuY-ZGXY",
        "colab_type": "text"
      },
      "source": [
        "## Load Inception-V3 model\n",
        "Load model and remove last layer so that features can be extracted from images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KvhmsIfZQFN",
        "colab_type": "code",
        "outputId": "7b43c2f3-baeb-4119-bf95-7f1ad0286e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Load Inception-V3 model\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "# Create new model, by removing last layer (output layer) from Inception-V3\n",
        "model_new = Model(inputs=model.input, outputs=model.layers[-2].output) # outputs=(second last layer output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fus9UoFaQuM",
        "colab_type": "text"
      },
      "source": [
        "## Extract feature from images\n",
        "Run images through Inception-V3 model to extract features as a vector. \n",
        "\n",
        "Each image will have dimensions of (2048, ), which is the required size for inputting into image captioning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDfA2jIeaPbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to extract features from image into a vector of size (2048, )\n",
        "def extract_image_features(image):\n",
        "    \"\"\"\n",
        "    Inputs      - image = image filename\n",
        "    Outputs     - feature_vector = output vector from Inception-V3\n",
        "    \"\"\"\n",
        "    # Pre-process image so it can be run through Inception-V3\n",
        "    image = preprocess_image(image)\n",
        "    \n",
        "    # Extract features from image by running it through Inception-V3\n",
        "    feature_vector = model_new.predict(image) \n",
        "\n",
        "    # Reshape from (1, 2048) to (2048, )\n",
        "    feature_vector = feature_vector.reshape(feature_vector.shape[1], ) \n",
        "\n",
        "    return feature_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GM5v0g6agYX",
        "colab_type": "code",
        "outputId": "986872ea-1136-46d3-c745-a85e8a7be63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "# Extract features from train images\n",
        "start_train = time()\n",
        "train_features = {}\n",
        "for idx,img in enumerate(train_img_filenames):\n",
        "    if( (idx+1)%500 == 0):\n",
        "        print('Train images encoded ',idx+1)\n",
        "    train_features[img] = extract_image_features(img)\n",
        "print(\"Train image feature extraction took: {} seconds\".format(time()-start_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images encoded  500\n",
            "Train images encoded  1000\n",
            "Train images encoded  1500\n",
            "Train images encoded  2000\n",
            "Train images encoded  2500\n",
            "Train images encoded  3000\n",
            "Train images encoded  3500\n",
            "Train images encoded  4000\n",
            "Train images encoded  4500\n",
            "Train images encoded  5000\n",
            "Train images encoded  5500\n",
            "Train images encoded  6000\n",
            "Train image feature extraction took: 3345.310848712921 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9OtOmbodFGG",
        "colab_type": "code",
        "outputId": "6ca942e8-7cc0-4b55-e0f2-a3c7b780c5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Extract features from validation images\n",
        "start_val = time()\n",
        "val_features = {}\n",
        "for idx,img in enumerate(val_img_filenames):\n",
        "    if( (idx+1)%500 == 0):\n",
        "        print('Validation images encoded ',idx+1)\n",
        "    val_features[img] = extract_image_features(img)\n",
        "print(\"Validation image feature extraction took: {} seconds\".format(time()-start_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation images encoded  500\n",
            "Validation images encoded  1000\n",
            "Validation image feature extraction took: 568.206071138382 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCbXmOL4IZAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract features from test images\n",
        "test_features = {}\n",
        "for idx,img in enumerate(test_img_filenames):\n",
        "    if( (idx+1)%500 == 0):\n",
        "        print('Test images encoded ',idx+1)\n",
        "    test_features[img] = extract_image_features(img)\n",
        "print(\"Test image feature extracted\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaxctjTdSu_u",
        "colab_type": "text"
      },
      "source": [
        "## Save feature vectors to pickle files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1RJkV63IYyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save train, val and test features as pickle file in working directory\n",
        "dump(train_features, open('train_features_inception.pkl', 'wb'))\n",
        "dump(val_features, open('val_features_inception.pkl', 'wb'))\n",
        "dump(test_features, open('test_features_inception.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ippPiTeh5F",
        "colab_type": "text"
      },
      "source": [
        "# Load image features and captions -- RUN\n",
        "Load image features from pickle files and get associated captions from caption dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO68DYD0UCtT",
        "colab_type": "text"
      },
      "source": [
        "## Load pre-processed captions\n",
        "Adds startseq, endseq to all captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_-dVFoWF5nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and read pre-processed captions text file\n",
        "def load_doc(filename):\n",
        "    \"\"\"\n",
        "    Inputs      - filename = .txt file with pre-processed captions\n",
        "    Outputs      - text = opened file\n",
        "    \"\"\"\n",
        "    with open(filename) as file:\n",
        "        text = file.readlines()\n",
        "        return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlCcUUZb1URw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionary with image id as key, captions as value\n",
        "# Add 'startseq' and 'endseq' to caption\n",
        "def load_captions(filename, filelist):\n",
        "    \"\"\"\n",
        "    Inputs      - filename = .txt file with pre-prcocessed captions\n",
        "                - filelist = list of images names for train/val/test\n",
        "    Outputs     - captions = dictionary with image id as key, pre-processed captions as values\n",
        "    \"\"\"\n",
        "\n",
        "    # Read in file\n",
        "    doc = load_doc(filename) \n",
        "    captions = {}    \n",
        "    \n",
        "    # Create dictionary with image id as key, captions as values\n",
        "    # Add startseq endseq to caption\n",
        "    for line in doc:\n",
        "        tokens = line.split()\n",
        "        image_id, image_caption = tokens[0], tokens[1:]\n",
        "\n",
        "        if(image_id in filelist):\n",
        "            if(image_id not in captions):\n",
        "                captions[image_id] = []\n",
        "            \n",
        "            modified_caption = 'startseq ' + ' '.join(image_caption) + ' endseq'\n",
        "            captions[image_id].append(modified_caption)\n",
        "    \n",
        "    return captions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f1MDlge1nSZ",
        "colab_type": "code",
        "outputId": "c52d1e00-0e6d-4ec9-dbe6-ca0be3afcba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Create train captions dictionary\n",
        "train_captions = load_captions('captions.txt', train_img_name)\n",
        "\n",
        "# Print a sample caption as a check\n",
        "# Should return 5 captions, each sandwiched between 'startseq' 'endseq'\n",
        "print(*train_captions['1000268201_693b08cb0e'],sep='\\n')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
            "startseq girl going into wooden building endseq\n",
            "startseq little girl climbing into wooden playhouse endseq\n",
            "startseq little girl climbing the stairs to her playhouse endseq\n",
            "startseq little girl in pink dress going into wooden cabin endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0torB_C133v",
        "colab_type": "code",
        "outputId": "f93160dc-5bf0-4ab8-9d9e-c475d1b3acdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Create val captions dictionary\n",
        "val_captions = load_captions('captions.txt', val_img_name)\n",
        "\n",
        "# Print a sample caption as a check\n",
        "# Should return 5 captions, each sandwiched between 'startseq' 'endseq'\n",
        "print(*val_captions['2391812384_7429b5e567'],sep='\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq child dressed in purple clothing and hat runs in splashing water while man and small boy walk nearby endseq\n",
            "startseq child wearing purple runs through splashing water while man in red looks on endseq\n",
            "startseq family playing the spray of fountain endseq\n",
            "startseq man and two toddlers get sprinkled as they pass an ornamental fountain endseq\n",
            "startseq young girl wearing purple running through water endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGFHVWmRIiUe",
        "colab_type": "code",
        "outputId": "27f9a853-4106-4ed2-f000-cbb36a0ebfc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Create test captions dictionary\n",
        "test_captions = load_captions('captions.txt', test_img_name)\n",
        "\n",
        "# Print a sample caption as a check\n",
        "# Should return 5 captions, each sandwiched between 'startseq' 'endseq'\n",
        "print(*test_captions['3385593926_d3e9c21170'],sep='\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq the dogs are in the snow in front of fence endseq\n",
            "startseq the dogs play on the snow endseq\n",
            "startseq two brown dogs playfully fight in the snow endseq\n",
            "startseq two brown dogs wrestle in the snow endseq\n",
            "startseq two dogs playing in the snow endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPf-1-4XULAY",
        "colab_type": "text"
      },
      "source": [
        "## Load pre-processed features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO9NpPEee9Ch",
        "colab_type": "code",
        "outputId": "2ac996cf-350d-4fdd-ea2f-344ce7ac22bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Load train pickle file\n",
        "train_features = load(open('train_features_inception.pkl', \"rb\"))\n",
        "\n",
        "# Create a list of all training captions\n",
        "all_train_captions = []\n",
        "for captions in train_captions.values():\n",
        "    for caption in captions:\n",
        "        all_train_captions.append(caption)\n",
        "\n",
        "# Check length of captions is correct\n",
        "assert len(all_train_captions) == 5 * len(train_img_name)\n",
        "\n",
        "# Check by printing 10 captions\n",
        "print(*all_train_captions[:10],sep='\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq child in pink dress is climbing up set of stairs in an entry way endseq\n",
            "startseq girl going into wooden building endseq\n",
            "startseq little girl climbing into wooden playhouse endseq\n",
            "startseq little girl climbing the stairs to her playhouse endseq\n",
            "startseq little girl in pink dress going into wooden cabin endseq\n",
            "startseq black dog and spotted dog are fighting endseq\n",
            "startseq black dog and dog playing with each other on the road endseq\n",
            "startseq black dog and white dog with brown spots are staring at each other in the street endseq\n",
            "startseq two dogs of different breeds looking at each other on the road endseq\n",
            "startseq two dogs on pavement moving toward each other endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_hlVFQMzN8Z",
        "colab_type": "code",
        "outputId": "4e8e6ee2-ba32-446c-e28e-369f75072afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Load val pickle file\n",
        "val_features = load(open('val_features_inception.pkl', \"rb\"))\n",
        "\n",
        "# Create a list of all validation captions\n",
        "all_val_captions = []\n",
        "for captions in val_captions.values():\n",
        "    for caption in captions:\n",
        "        all_val_captions.append(caption)\n",
        "\n",
        "# Check length of captions is correct  \n",
        "assert len(all_val_captions) == 5 * len(val_img_name)\n",
        "\n",
        "# Check by printing 10 captions\n",
        "print(*all_val_captions[:10],sep='\\n')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq child and woman are at waters edge in big city endseq\n",
            "startseq large lake with lone duck swimming in it with several people around the edge of it endseq\n",
            "startseq little boy at lake watching duck endseq\n",
            "startseq young boy waves his hand at the duck in the water surrounded by green park endseq\n",
            "startseq two people are at the edge of lake facing the water and the city skyline endseq\n",
            "startseq boy with stick kneeling in front of goalie net endseq\n",
            "startseq child in red jacket playing street hockey guarding goal endseq\n",
            "startseq young kid playing the goalie in hockey rink endseq\n",
            "startseq young male kneeling in front of hockey goal with hockey stick in his right hand endseq\n",
            "startseq hockey goalie boy in red jacket crouches by goal with stick endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gokv9eJKIox4",
        "colab_type": "code",
        "outputId": "5c7b1953-e255-459c-872f-16a9f58a5e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Load test pickle file\n",
        "test_features = load(open('test_features_inception.pkl', \"rb\"))\n",
        "\n",
        "# Create a list of all test captions\n",
        "all_test_captions = []\n",
        "for captions in test_captions.values():\n",
        "    for caption in captions:\n",
        "        all_test_captions.append(caption)\n",
        "\n",
        "# Check length of captions is correct  \n",
        "assert len(all_test_captions) == 5 * len(test_img_name)\n",
        "\n",
        "# Check by printing 10 captions\n",
        "print(*all_test_captions[:10],sep='\\n')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq blond woman in blue shirt appears to wait for ride endseq\n",
            "startseq blond woman is on the street hailing taxi endseq\n",
            "startseq woman is signaling is to traffic as seen from behind endseq\n",
            "startseq woman with blonde hair wearing blue tube top is waving on the side of the street endseq\n",
            "startseq the woman in the blue dress is holding out her arm at oncoming traffic endseq\n",
            "startseq boy in his blue swim shorts at the beach endseq\n",
            "startseq boy smiles for the camera at beach endseq\n",
            "startseq young boy in swimming trunks is walking with his arms outstretched on the beach endseq\n",
            "startseq children playing on the beach endseq\n",
            "startseq the boy is playing on the shore of an ocean endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEkDsO3Zfg_e",
        "colab_type": "text"
      },
      "source": [
        "#  Create vocab -- RUN\n",
        "Select threshold for number of times a word must appear in training captions for word to be added to vocab.\n",
        "\n",
        "Tokenize captions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd67hyIZfs2C",
        "colab_type": "code",
        "outputId": "ae4f72c9-4e75-4570-c9a2-e02425995f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Set threshold for number of times word must appear in captions to be included in vocab\n",
        "corpus_threshold = 5\n",
        "\n",
        "# Create corpus by tokenizing all training captions\n",
        "corpus = []\n",
        "for caption in all_train_captions:\n",
        "    for token in caption.split():\n",
        "        corpus.append(token)\n",
        "        \n",
        "hash_map = Counter(corpus)\n",
        "\n",
        "# Create vocab by only including words from corpus that appears enough times in captions\n",
        "vocab = []\n",
        "for token,count in hash_map.items():\n",
        "    if(count>=corpus_threshold):\n",
        "        vocab.append(token)\n",
        "        \n",
        "print('Number of original tokens:', len(hash_map))\n",
        "print('Number of tokens after threshold:', len(vocab))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of original tokens: 7265\n",
            "Number of tokens after threshold: 2503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13UWt-9dgb5N",
        "colab_type": "text"
      },
      "source": [
        "## Create word-to-index and index-to-word dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fCQ4O0igaUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "    \n",
        "for idx,token in enumerate(vocab):\n",
        "    word_to_index[token] = idx+1\n",
        "    index_to_word[idx+1] = token\n",
        "\n",
        "vocab_size = len(index_to_word) + 1 # Add for appended zeros\n",
        "\n",
        "# Check lengths are identical\n",
        "assert (len(index_to_word)) == (len(index_to_word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_yydJWLqfT6",
        "colab_type": "text"
      },
      "source": [
        "## Find max length caption\n",
        "Need to know max length caption to set padding size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diFqLseKql2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_len_caption(all_train_captions):\n",
        "    \"\"\"\n",
        "    Inputs      - all_train_captions = list of all train captions\n",
        "    Outputs     - max_len = integer\n",
        "    \"\"\"\n",
        "    max_len = 0\n",
        "    for caption in all_train_captions:\n",
        "        max_len = max(max_len,len(caption.split()))\n",
        "    print('Maximum length of caption:',max_len)\n",
        "    return max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwbyw4fNqomq",
        "colab_type": "code",
        "outputId": "c377e1c1-144f-44a2-c4a8-0658ffae76d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Determine maximum caption length\n",
        "max_length_caption = max_len_caption(all_train_captions)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum length of caption: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7-zvKB7-lRY",
        "colab_type": "text"
      },
      "source": [
        "# Data generator\n",
        "\n",
        "Code has been adapted from Jeff Heaton's GitHub repository *T81-588 Applications of Deep Neural Networks*, <https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_4_captioning.ipynb>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk5CNArk-knF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data generator with required input for image captioning model\n",
        "def data_generator(descriptions, photos, wordtoidx, max_length, num_photos_per_batch):\n",
        "    \"\"\"\n",
        "    Inputs      - descriptions = captions dictionary\n",
        "                - photos = features vector\n",
        "                - wordtoix = word to index dictionary\n",
        "                - max_length = maximum length of caption, outut from max_len_caption()\n",
        "                - num_photos_per_batch = integer\n",
        "    Outputs     - X1 = numpy array of image features\n",
        "                - X2 = numpy array of caption sequences\n",
        "                - y = numpy array of next word in caption\n",
        "    \"\"\"\n",
        "    X1, X2, y = [], [], []\n",
        "    n=0\n",
        "    while True:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            temp = '/content/drive/Shared drives/DL_AT3/Experimentation/Flicker8k_Dataset/'\n",
        "            photo = photos[temp + key +'.jpg']\n",
        "            for desc in desc_list:\n",
        "                seq = [wordtoidx[word] for word in desc.split(' ') if word in wordtoidx]\n",
        "                for i in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n==num_photos_per_batch:\n",
        "                yield ([np.array(X1), np.array(X2)], np.array(y))\n",
        "                X1, X2, y = [], [], []\n",
        "                n=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB4VzDpfro-J",
        "colab_type": "text"
      },
      "source": [
        "## Load GloVe vectors\n",
        "Source: <https://nlp.stanford.edu/projects/glove/>\n",
        "\n",
        "Used to create the embedding matrix for caption input encoder. \n",
        "\n",
        "Will match words that are similar, for example frog and toad.\n",
        "\n",
        "glove.6B.200d.txt must be located in working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuO8-EDnrzE0",
        "colab_type": "code",
        "outputId": "9785b8c5-f435-4e4c-d3e3-62b4153e117c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load GloVe file\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open('glove.6B.200d.txt', encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors' % len(embeddings_index))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47NhXcV4sj-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create embedding matrix\n",
        "embedding_dim = 200\n",
        "\n",
        "# Create 200 dim dense vector for each of the words in vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in word_to_index.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Check shape size        \n",
        "assert embedding_matrix.shape == (vocab_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hViZWTkBtKYQ",
        "colab_type": "text"
      },
      "source": [
        "# Image caption model -- RUN\n",
        "Based on merge-model described by Tanti et al. in *Where to put the Image in an Image Caption Generator*\n",
        "\n",
        "source: <https://arxiv.org/abs/1703.09137>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYulvaUpBsu6",
        "colab_type": "text"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBR1BMpWtJzT",
        "colab_type": "code",
        "outputId": "20492d50-0cf6-46a8-fb19-a4f2320a1e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "# Feature extractor model\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "# Sequence model\n",
        "inputs2 = Input(shape=(max_length_caption,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "# Decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "# Entire model\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 33, 200)      500800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 33, 200)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2504)         643528      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,202,632\n",
            "Trainable params: 2,202,632\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPVgDXjLBwq6",
        "colab_type": "text"
      },
      "source": [
        "## Set final layers weights to equal embedding matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4rNo9agutsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use GloVe embedding matric as weights for second layer of caption encoder\n",
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVdphpd-B1vC",
        "colab_type": "text"
      },
      "source": [
        "## Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2sBLZQTuyTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile model using adam and categorical cross entropy loss\n",
        "# Change learning rate as necessary\n",
        "learning_rate = 0.001\n",
        "optimizer = optimizers.Adam(learning_rate = learning_rate)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py49JcYUaSqh",
        "colab_type": "text"
      },
      "source": [
        "# Train model -- RUN\n",
        "Use checkpoing callbacks to save training information.\n",
        "\n",
        "Change batch size as required.\n",
        "\n",
        "Epochs are set in `model.fit()`, change as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at8sHx6ETlLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set number of pictures per batch \n",
        "number_pics_per_batch = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvCe_HuDS--t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data generators\n",
        "train_generator = data_generator(train_captions, train_features, word_to_index, max_length_caption, number_pics_per_batch)\n",
        "val_generator = data_generator(val_captions, val_features, word_to_index, max_length_caption, number_pics_per_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAJOfWS2B8ut",
        "colab_type": "text"
      },
      "source": [
        "## Set callbacks\n",
        "Make sure you set `check_path` correctly!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Wu_OSEYEfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set callbacks\n",
        "#### MAKE A DIRECTORY TO SAVE CHECKPOINTS TO ##############################\n",
        "# eg: check_path = 'checkpoints/rebecca/exp_2'\n",
        "###########################################################################\n",
        "\n",
        "check_path = 'checkpoints/rebecca/final' # CHANGE THIS\n",
        "!mkdir -p $check_path\n",
        "\n",
        "check_dir = check_path+'/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\n",
        "# Monitor validation loss\n",
        "checkpoint = ModelCheckpoint(check_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# Use early stopping, change patience as required\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Use reduce LR, change patience and factor as required\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNRo77z0CDe9",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibTtm4EWYMS_",
        "colab_type": "code",
        "outputId": "6ecacfa3-b002-4fa5-9964-f185105b6df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit model\n",
        "tic = time.perf_counter()\n",
        "\n",
        "# Change epochs as necessary\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_img_name) // number_pics_per_batch,\n",
        "    epochs=100,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=len(val_img_name) // number_pics_per_batch ,\n",
        "    callbacks = [checkpoint, early_stopping, reduce_lr])\n",
        "\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-38-ae04ee305614>:11: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 4.9685\n",
            "Epoch 00001: val_loss improved from inf to 4.19862, saving model to checkpoints/rebecca/final/model-ep001-loss4.968-val_loss4.199.h5\n",
            "187/187 [==============================] - 39s 209ms/step - loss: 4.9685 - val_loss: 4.1986 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.9861\n",
            "Epoch 00002: val_loss improved from 4.19862 to 3.77645, saving model to checkpoints/rebecca/final/model-ep002-loss3.986-val_loss3.776.h5\n",
            "187/187 [==============================] - 39s 211ms/step - loss: 3.9861 - val_loss: 3.7765 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.6445\n",
            "Epoch 00003: val_loss improved from 3.77645 to 3.58548, saving model to checkpoints/rebecca/final/model-ep003-loss3.645-val_loss3.585.h5\n",
            "187/187 [==============================] - 39s 209ms/step - loss: 3.6445 - val_loss: 3.5855 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.4479\n",
            "Epoch 00004: val_loss improved from 3.58548 to 3.48788, saving model to checkpoints/rebecca/final/model-ep004-loss3.448-val_loss3.488.h5\n",
            "187/187 [==============================] - 39s 208ms/step - loss: 3.4479 - val_loss: 3.4879 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.3088\n",
            "Epoch 00005: val_loss improved from 3.48788 to 3.43548, saving model to checkpoints/rebecca/final/model-ep005-loss3.309-val_loss3.435.h5\n",
            "187/187 [==============================] - 39s 208ms/step - loss: 3.3088 - val_loss: 3.4355 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.2062\n",
            "Epoch 00006: val_loss improved from 3.43548 to 3.41401, saving model to checkpoints/rebecca/final/model-ep006-loss3.206-val_loss3.414.h5\n",
            "187/187 [==============================] - 39s 207ms/step - loss: 3.2062 - val_loss: 3.4140 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.1221\n",
            "Epoch 00007: val_loss improved from 3.41401 to 3.37618, saving model to checkpoints/rebecca/final/model-ep007-loss3.122-val_loss3.376.h5\n",
            "187/187 [==============================] - 39s 209ms/step - loss: 3.1221 - val_loss: 3.3762 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 3.0496\n",
            "Epoch 00008: val_loss improved from 3.37618 to 3.36281, saving model to checkpoints/rebecca/final/model-ep008-loss3.050-val_loss3.363.h5\n",
            "187/187 [==============================] - 40s 213ms/step - loss: 3.0496 - val_loss: 3.3628 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.9880\n",
            "Epoch 00009: val_loss improved from 3.36281 to 3.35852, saving model to checkpoints/rebecca/final/model-ep009-loss2.988-val_loss3.359.h5\n",
            "187/187 [==============================] - 39s 207ms/step - loss: 2.9880 - val_loss: 3.3585 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.9332\n",
            "Epoch 00010: val_loss did not improve from 3.35852\n",
            "187/187 [==============================] - 39s 207ms/step - loss: 2.9332 - val_loss: 3.3586 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.8830\n",
            "Epoch 00011: val_loss improved from 3.35852 to 3.34299, saving model to checkpoints/rebecca/final/model-ep011-loss2.883-val_loss3.343.h5\n",
            "187/187 [==============================] - 39s 209ms/step - loss: 2.8830 - val_loss: 3.3430 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.8367\n",
            "Epoch 00012: val_loss improved from 3.34299 to 3.33829, saving model to checkpoints/rebecca/final/model-ep012-loss2.837-val_loss3.338.h5\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.8367 - val_loss: 3.3383 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.7953\n",
            "Epoch 00013: val_loss improved from 3.33829 to 3.32480, saving model to checkpoints/rebecca/final/model-ep013-loss2.795-val_loss3.325.h5\n",
            "187/187 [==============================] - 39s 208ms/step - loss: 2.7953 - val_loss: 3.3248 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.7555\n",
            "Epoch 00014: val_loss did not improve from 3.32480\n",
            "187/187 [==============================] - 38s 204ms/step - loss: 2.7555 - val_loss: 3.3397 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.7247\n",
            "Epoch 00015: val_loss did not improve from 3.32480\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.7247 - val_loss: 3.3450 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.6694\n",
            "Epoch 00016: val_loss did not improve from 3.32480\n",
            "187/187 [==============================] - 37s 200ms/step - loss: 2.6694 - val_loss: 3.3534 - lr: 5.0000e-04\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.6375\n",
            "Epoch 00017: val_loss did not improve from 3.32480\n",
            "187/187 [==============================] - 38s 202ms/step - loss: 2.6375 - val_loss: 3.3552 - lr: 5.0000e-04\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.6130\n",
            "Epoch 00018: val_loss improved from 3.32480 to 3.31484, saving model to checkpoints/rebecca/final/model-ep018-loss2.613-val_loss3.315.h5\n",
            "187/187 [==============================] - 38s 204ms/step - loss: 2.6130 - val_loss: 3.3148 - lr: 2.5000e-04\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5967\n",
            "Epoch 00019: val_loss did not improve from 3.31484\n",
            "187/187 [==============================] - 38s 204ms/step - loss: 2.5967 - val_loss: 3.3189 - lr: 2.5000e-04\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5833\n",
            "Epoch 00020: val_loss did not improve from 3.31484\n",
            "187/187 [==============================] - 38s 202ms/step - loss: 2.5833 - val_loss: 3.3187 - lr: 2.5000e-04\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5730\n",
            "Epoch 00021: val_loss improved from 3.31484 to 3.29408, saving model to checkpoints/rebecca/final/model-ep021-loss2.573-val_loss3.294.h5\n",
            "187/187 [==============================] - 38s 204ms/step - loss: 2.5730 - val_loss: 3.2941 - lr: 1.2500e-04\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5633\n",
            "Epoch 00022: val_loss did not improve from 3.29408\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5633 - val_loss: 3.2981 - lr: 1.2500e-04\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5531\n",
            "Epoch 00023: val_loss improved from 3.29408 to 3.29234, saving model to checkpoints/rebecca/final/model-ep023-loss2.553-val_loss3.292.h5\n",
            "187/187 [==============================] - 38s 205ms/step - loss: 2.5531 - val_loss: 3.2923 - lr: 1.2500e-04\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5472\n",
            "Epoch 00024: val_loss improved from 3.29234 to 3.28984, saving model to checkpoints/rebecca/final/model-ep024-loss2.547-val_loss3.290.h5\n",
            "187/187 [==============================] - 39s 206ms/step - loss: 2.5472 - val_loss: 3.2898 - lr: 1.2500e-04\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5416\n",
            "Epoch 00025: val_loss did not improve from 3.28984\n",
            "187/187 [==============================] - 38s 202ms/step - loss: 2.5416 - val_loss: 3.2979 - lr: 1.2500e-04\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5335\n",
            "Epoch 00026: val_loss did not improve from 3.28984\n",
            "187/187 [==============================] - 38s 202ms/step - loss: 2.5335 - val_loss: 3.2991 - lr: 1.2500e-04\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5308\n",
            "Epoch 00027: val_loss improved from 3.28984 to 3.28707, saving model to checkpoints/rebecca/final/model-ep027-loss2.531-val_loss3.287.h5\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5308 - val_loss: 3.2871 - lr: 6.2500e-05\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5276\n",
            "Epoch 00028: val_loss improved from 3.28707 to 3.28251, saving model to checkpoints/rebecca/final/model-ep028-loss2.528-val_loss3.283.h5\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5276 - val_loss: 3.2825 - lr: 6.2500e-05\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5216\n",
            "Epoch 00029: val_loss did not improve from 3.28251\n",
            "187/187 [==============================] - 38s 204ms/step - loss: 2.5216 - val_loss: 3.2853 - lr: 6.2500e-05\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5205\n",
            "Epoch 00030: val_loss did not improve from 3.28251\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5205 - val_loss: 3.2882 - lr: 6.2500e-05\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5168\n",
            "Epoch 00031: val_loss improved from 3.28251 to 3.27680, saving model to checkpoints/rebecca/final/model-ep031-loss2.517-val_loss3.277.h5\n",
            "187/187 [==============================] - 39s 211ms/step - loss: 2.5168 - val_loss: 3.2768 - lr: 3.1250e-05\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5134\n",
            "Epoch 00032: val_loss did not improve from 3.27680\n",
            "187/187 [==============================] - 38s 205ms/step - loss: 2.5134 - val_loss: 3.2777 - lr: 3.1250e-05\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5094\n",
            "Epoch 00033: val_loss improved from 3.27680 to 3.27409, saving model to checkpoints/rebecca/final/model-ep033-loss2.509-val_loss3.274.h5\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5094 - val_loss: 3.2741 - lr: 3.1250e-05\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5085\n",
            "Epoch 00034: val_loss did not improve from 3.27409\n",
            "187/187 [==============================] - 39s 206ms/step - loss: 2.5085 - val_loss: 3.2793 - lr: 3.1250e-05\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5091\n",
            "Epoch 00035: val_loss improved from 3.27409 to 3.27288, saving model to checkpoints/rebecca/final/model-ep035-loss2.509-val_loss3.273.h5\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5091 - val_loss: 3.2729 - lr: 3.1250e-05\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5039\n",
            "Epoch 00036: val_loss improved from 3.27288 to 3.27189, saving model to checkpoints/rebecca/final/model-ep036-loss2.504-val_loss3.272.h5\n",
            "187/187 [==============================] - 39s 206ms/step - loss: 2.5039 - val_loss: 3.2719 - lr: 3.1250e-05\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5036\n",
            "Epoch 00037: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 201ms/step - loss: 2.5036 - val_loss: 3.2803 - lr: 3.1250e-05\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5023\n",
            "Epoch 00038: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5023 - val_loss: 3.2772 - lr: 3.1250e-05\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.5006\n",
            "Epoch 00039: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.5006 - val_loss: 3.2794 - lr: 1.5625e-05\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4990\n",
            "Epoch 00040: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 205ms/step - loss: 2.4990 - val_loss: 3.2733 - lr: 1.5625e-05\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4960\n",
            "Epoch 00041: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 39s 206ms/step - loss: 2.4960 - val_loss: 3.2754 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4946\n",
            "Epoch 00042: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 204ms/step - loss: 2.4946 - val_loss: 3.2789 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4937\n",
            "Epoch 00043: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 202ms/step - loss: 2.4937 - val_loss: 3.2749 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4908\n",
            "Epoch 00044: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 202ms/step - loss: 2.4908 - val_loss: 3.2789 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4926\n",
            "Epoch 00045: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 203ms/step - loss: 2.4926 - val_loss: 3.2782 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - ETA: 0s - loss: 2.4912\n",
            "Epoch 00046: val_loss did not improve from 3.27189\n",
            "187/187 [==============================] - 38s 205ms/step - loss: 2.4912 - val_loss: 3.2810 - lr: 1.0000e-05\n",
            "Model ran in: 29.63 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08SYdZ2xClny",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate image caption model -- RUN\n",
        "Plot loss curves.\n",
        "\n",
        "Calculate BLEU scores for validation and test set (if appropriate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YZh347eCI3K",
        "colab_type": "text"
      },
      "source": [
        "## Loss curves\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfuWrMh3wAV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelPlot(history_name, plt_title):\n",
        "    \"\"\"\n",
        "    Generates plot (with title) for model training and validation accuracy and loss by epoch\n",
        "    Inputs  - history_name = history object resulting from fitted model\n",
        "            - plt_title = title for plot, as string\n",
        "    Output  - plot\n",
        "    \"\"\"\n",
        "    from matplotlib.pyplot import savefig\n",
        "    loss_filename = check_path+'/loss.png'\n",
        "\n",
        "    plt.plot(history_name.history['loss'], label='Training', color = 'blue')\n",
        "    plt.plot(history_name.history['val_loss'], label='Validation', color = 'orange')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title(plt_title)\n",
        "    plt.legend()\n",
        "    plt.savefig(loss_filename)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMbbODpwBk_",
        "colab_type": "code",
        "outputId": "646bb216-955f-4426-f7fb-0842f724a77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# Change title\n",
        "title = 'Final Model' # CHANGE THIS\n",
        "\n",
        "# Plot will save to check_path as loss.png\n",
        "modelPlot(history, title)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU9b3/8dcnCwkQ9l0CBlTABQkQl4oKaheqFFtFK9pWaqtVe7XaWlv9tbXa29vrrdeqt9rWui8VW63WtVZR1Cu9KntFQFFRUDbDFtZsn98f3zPJJARIQmYmyXk/H4/zOGfOnJn55Cjznu/5nvM95u6IiEh8ZWW6ABERySwFgYhIzCkIRERiTkEgIhJzCgIRkZhTEIiIxJyCQNo9M9tiZkNb4H1+bmYPtERNqfhMM5tpZt9OdU3S/igIpN0ws+Vmtj364k9M+7l7gbu/n+LPnmBmbmaP1Vs/Klo/M5WfL7IvFATS3nwp+uJPTJ+k8bPXAZ8xs15J684F3kljDSJNpiCQdi/6RX5gtHyPmd1qZk+bWZmZvW5mByRte7OZrTCzzWY2x8yOa8JHlQOPA2dF75UNfBV4sF49x5jZm2a2KZofk/TcEDN7OarteaB3vdcebWazzGyjmS0wswlN3B0iu1AQSBydBVwL9ACWAb9Meu5NoBjoCfwJ+IuZ5Tfhve8DvhEtfwF4C6hplZhZT+Bp4BagF3Aj8HRSK+JPwBxCAPyC0KJIvHZg9Np/j+q7AnjUzPo0oT6RXSgIpL15PPq1vNHMHt/NNo+5+xvuXkn4tV6ceMLdH3D3UnevdPf/BvKA4Y39cHefBfQ0s+GEQLiv3ianAO+6+/3RZzwELAG+ZGaDgSOAn7r7Tnd/BXgy6bVfA55x92fcvdrdnwdmAyc3tj6RhigIpL35srt3j6Yv72ab1UnL24CCxAMzu8LMFkeHbTYC3ah3eKYR7gf+DTgBeKzec/sBH9Zb9yEwMHpug7tvrfdcwv7AGUlBtxE4FhjQxPpE6sjJdAEirUXUH3AlcBKwyN2rzWwDYE18q/sJh5zuc/dtZnVe/gnhCz3ZYODvwCqgh5l1TgqDwUBiiOAVwP3ufn4T6xHZI7UIRGp1ASoJZ//kmNnPgK5NfRN3/wAYD/y/Bp5+BhhmZmebWY6ZfRU4BHjK3T8kHOq51sw6mNmxwJeSXvsA4RDSF8ws28zyo9NWC5tao0gyBYFIrecIv8zfIRyS2UH4Fd5k7v6/DZ266u6lwCTgB0ApoQUyyd0/jTY5GzgKWA9cQ1Ifg7uvAE4FriaE1Qrgh+jfsewj041pRETiTb8kRERiTkEgIhJzCgIRkZhTEIiIxFybu46gd+/eXlRUlOkyRETalDlz5nzq7g0OR9LmgqCoqIjZs2dnugwRkTbFzOpf0V5Dh4ZERGJOQSAiEnMKAhGRmEtpH4GZLQfKgCqg0t1L6j1vwM2EYXS3AdPcfW4qaxKR1qOiooKVK1eyY8eOTJfSbuTn51NYWEhubm6jX5OOzuITksZRqe+LwEHRdBTwu2guIjGwcuVKunTpQlFREfVGaZVmcHdKS0tZuXIlQ4YMafTrMn1o6FTCUL3u7v8HdDczja0uEhM7duygV69eCoEWYmb06tWryS2sVAeBA/+I7v16QQPPD6Tu6I4ro3V1mNkFZjbbzGavW7cuRaWKSCYoBFpWc/ZnqoPgWHcfQzgE9F0zO745b+Lut7t7ibuX9OnTvNuzvvUW/OQnUFrarJeLiLRbKQ0Cd/84mq8l3LLvyHqbfAwMSnpcGK1rce++C7/8JXz0USreXUTaotLSUoqLiykuLqZ///4MHDiw5nF5efkeXzt79mwuvfTSvX7GMccc01LlpkzKOovNrDOQ5e5l0fLngevqbfYE8G9mNp3QSbzJ3Velop6+fcN87dpUvLuItEW9evVi/vz5APz85z+noKCAK664oub5yspKcnIa/posKSmhpKSkweeSzZo1q2WKTaFUtgj6Af9rZguAN4Cn3f3vZnahmV0YbfMM8D7h/q5/BC5OVTEKAhFpjGnTpnHhhRdy1FFHceWVV/LGG2/wmc98htGjR3PMMcewdOlSAGbOnMmkSZOAECLnnXceEyZMYOjQodxyyy0171dQUFCz/YQJE5gyZQojRozgnHPOIXFjsGeeeYYRI0YwduxYLr300pr3TZeUtQjc/X1gVAPrf5+07MB3U1VDMgWBSOt22WUQ/ThvMcXFcNNNTX/dypUrmTVrFtnZ2WzevJlXX32VnJwcXnjhBa6++moeffTRXV6zZMkSXnrpJcrKyhg+fDgXXXTRLufyz5s3j0WLFrHffvsxbtw4XnvtNUpKSvjOd77DK6+8wpAhQ5g6dWpz/9xma3ODzjVX166Ql6cgEJG9O+OMM8jOzgZg06ZNnHvuubz77ruYGRUVFQ2+5pRTTiEvL4+8vDz69u3LmjVrKCwsrLPNkUceWbOuuLiY5cuXU1BQwNChQ2vO+586dSq33357Cv+6XcUmCMxCq2DNmkxXIiINac4v91Tp3LlzzfJPf/pTTjjhBB577DGWL1/OhAkTGnxNXl5ezXJ2djaVlZXN2iYTMn1BWVr17asWgYg0zaZNmxg4MFzedM8997T4+w8fPpz333+f5cuXA/Dwww+3+GfsjYJARGQPrrzySq666ipGjx6dkl/wHTt25LbbbmPixImMHTuWLl260K1btxb/nD2xRK91W1FSUuLNvTHNtGnw4ou6lkCktVi8eDEHH3xwpsvIuC1btlBQUIC7893vfpeDDjqIyy+/vNnv19B+NbM59Qf+TIhli6CNZZ+ItHN//OMfKS4u5tBDD2XTpk185zvfSevnx6azGEIQ7NwJZWXhLCIRkdbg8ssv36cWwL6KXYsA1E8gIpJMQSAiEnMKAhGRmFMQiIjEXCyDQFcXiwjACSecwHPPPVdn3U033cRFF13U4PYTJkwgcfr6ySefzMaNG3fZ5uc//zk33HDDHj/38ccf5+233655/LOf/YwXXnihqeW3mFgFQYcO0L27WgQiEkydOpXp06fXWTd9+vRGDfz2zDPP0L1792Z9bv0guO666/jsZz/brPdqCbEKAtDVxSJSa8qUKTz99NM1N6FZvnw5n3zyCQ899BAlJSUceuihXHPNNQ2+tqioiE8//RSAX/7ylwwbNoxjjz22ZphqCNcHHHHEEYwaNYrTTz+dbdu2MWvWLJ544gl++MMfUlxczHvvvce0adN45JFHAJgxYwajR49m5MiRnHfeeezcubPm86655hrGjBnDyJEjWbJkSYvth1hdRwAKApFWa85lsKGFx6HuUQxjdz+aXc+ePTnyyCN59tlnOfXUU5k+fTpnnnkmV199NT179qSqqoqTTjqJhQsXcvjhhzdc9pw5TJ8+nfnz51NZWcmYMWMYO3YsAKeddhrnn38+AD/5yU+48847ueSSS5g8eTKTJk1iypQpdd5rx44dTJs2jRkzZjBs2DC+8Y1v8Lvf/Y7LLrsMgN69ezN37lxuu+02brjhBu64446W2EtqEYhIvCUfHkocFvrzn//MmDFjGD16NIsWLapzGKe+V199la985St06tSJrl27Mnny5Jrn3nrrLY477jhGjhzJgw8+yKJFi/ZYy9KlSxkyZAjDhg0D4Nxzz+WVV16pef60004DYOzYsTWD1LWEWLYIkvariLQWe/jlnkqnnnoql19+OXPnzmXbtm307NmTG264gTfffJMePXowbdo0duzY0az3njZtGo8//jijRo3innvuYebMmftUa2IY65YewjqWLYLSUmglw4CLSIYVFBRwwgkncN555zF16lQ2b95M586d6datG2vWrOHZZ5/d4+uPP/54Hn/8cbZv305ZWRlPPvlkzXNlZWUMGDCAiooKHnzwwZr1Xbp0oaysbJf3Gj58OMuXL2fZsmUA3H///YwfP76F/tLdi2UQuIcwEBGBcHhowYIFTJ06lVGjRjF69GhGjBjB2Wefzbhx4/b42jFjxvDVr36VUaNG8cUvfpEjjjii5rlf/OIXHHXUUYwbN44RI0bUrD/rrLP49a9/zejRo3nvvfdq1ufn53P33XdzxhlnMHLkSLKysrjwwgtJtVgNQw3wl7/AmWfCwoUwcmQLFiYiTaZhqFNDw1Dvha4uFhGpK7ZBoKuLRUSC2AVBv35hrhaBSOvQ1g5Pt3bN2Z+xC4Lu3SEnR0Eg0hrk5+dTWlqqMGgh7k5paSn5+flNel3sriPIyoI+fRQEIq1BYWEhK1euZN26dZkupd3Iz8+nsLCwSa+JXRCAri4WaS1yc3MZMmRIpsuIvdgdGgIFgYhIMgWBiEjMKQhERGIutkGwdWuYRETiLuVBYGbZZjbPzJ5q4LlpZrbOzOZH07dTXQ/UXlSmExVERNJz1tD3gMVA1908/7C7/1sa6qiRPMxEUVE6P1lEpPVJaYvAzAqBU4CWuY1OC0lcXaxhJkREUn9o6CbgSqB6D9ucbmYLzewRMxuU4noADTwnIpIsZUFgZpOAte4+Zw+bPQkUufvhwPPAvbt5rwvMbLaZzW6JKxD79AlzBYGISGpbBOOAyWa2HJgOnGhmDyRv4O6l7r4zengHMLahN3L32929xN1L+iS+xfdBp05QUKAgEBGBFAaBu1/l7oXuXgScBbzo7l9L3sbMBiQ9nEzoVE4LXUsgIhKkfawhM7sOmO3uTwCXmtlkoBJYD0xLVx0KAhGRIC1B4O4zgZnR8s+S1l8FXJWOGurr2xeWL8/EJ4uItC6xvLIY1CIQEUmIdRCsWwfVezqxVUQkBmIdBFVVsGFDpisREcmsWAcB6OpiEZHYBoFuYi8iEsQ2CDTMhIhIoCBQEIhIzMU2CHr1AjMFgYhIbIMgOxt691YQiIjENghAF5WJiICCQEEgIrGnIFAQiEjMKQgUBCISc7EPgk2bYMeOTFciIpI5sQ8CCIPPiYjEVayDQMNMiIjEPAh0dbGISJyCwB22fgheewMCBYGISJyC4IP74W9FULasZpWCQEQkTkHQY1SYr59Ts6qgAPLzFQQiEm/xCYJuh0B2PqyfXbPKTNcSiIjEJwiycqH7qDotAlAQiIjEJwgAepbA+rm7dBgrCEQkzmIWBGOhsgzK3q1ZpSAQkbiLXxBAncNDiSBwz1BNIiIZFq8gSHQYl9Z2GPftC+XlYcwhEZE4ilcQZOVA92LYUNsi0DATIhJ38QoCgF51O4x1UZmIxF38gqDnWKjcApvfARQEIiLxDAKoubBMQSAicRe/IOh6MGR3rDlzqHfvsFpBICJxlfIgMLNsM5tnZk818FyemT1sZsvM7HUzK0p1PWTlQI/imiDo0AF69FAQiEh8paNF8D1g8W6e+xawwd0PBH4DXJ+GesIVxhvmQnUVoIvKRCTeUhoEZlYInALcsZtNTgXujZYfAU4yM0tlTUDUYbwVymo7jBUEIhJXqW4R3ARcCVTv5vmBwAoAd68ENgG9UlxTgx3Ga9ak/FNFRFqllAWBmU0C1rr7nL1uvPf3usDMZpvZ7HUtcaf5riMgu1NNP0H//vDxx1C9u7gSEWnHUtkiGAdMNrPlwHTgRDN7oN42HwODAMwsB+gGlNZ/I3e/3d1L3L2kT58++15ZTYdxaBGUlEBZGSxatO9vLSLS1qQsCNz9KncvdPci4CzgRXf/Wr3NngDOjZanRNukZ/i3niWwfh5UVzF+fFj18stp+WQRkVYl7dcRmNl1ZjY5engn0MvMlgHfB36ctkJ6joWqbVC2lKIiGDwYZs5M26eLiLQaOen4EHefCcyMln+WtH4HcEY6athFosO4dDbW7RDGj4dnnw3DUafhvCURkVYjflcWJ9TrMJ4wAT79FN5+O7NliYikW3yDICsbeo6u6TCeMCGsVj+BiMRNfIMAoiuM50N1JUOGQGGh+glEJH5iHgRRh/HmJZiFVsHLL+u2lSISLwoCqOknGD8+DDWxZEkGaxIRSbN4B0GX4ZDTuU6HMaifQETiJd5BkJUNPWo7jA84APbbT/0EIhIv8Q4CqNNhrH4CEYmjRgWBmXU2s6xoeZiZTTaz3NSWliY9x0LVdtgcbpkwfjysXg3vvJPhukRE0qSxLYJXgHwzGwj8A/g6cE+qikqreh3GiX4CHR4SkbhobBCYu28DTgNuc/czgENTV1YadRkGOQVQGvoJDjoIBgxQh7GIxEejg8DMPgOcAzwdrctOTUlpVtNhHFoEZuHw0MyZ6icQkXhobBBcBlwFPObui8xsKPBS6spKsz7HwIY5sH0VEA4PrVoFy5ZltiwRkXRoVBC4+8vuPtndr486jT9190tTXFv6HPBtqK6Ed24FqLk/gfoJRCQOGnvW0J/MrKuZdQbeAt42sx+mtrQ06nIgFJ4Ky34PldsYPhz69VM/gYjEQ2MPDR3i7puBLwPPAkMIZw61HyMuh52l8MH96icQkVhpbBDkRtcNfBl4wt0rgPb1FdnnuHAq6dLfgFczYUK4of3772e6MBGR1GpsEPwBWA50Bl4xs/2BzakqKiPMYMT3YfNS+OTv6icQkdhobGfxLe4+0N1P9uBD4IQU15Z+g8+AjgNhyY0cfDD06aN+AhFp/xrbWdzNzG40s9nR9N+E1kH7kpULwy+BNTOwjQuYMEH9BCLS/jX20NBdQBlwZjRtBu5OVVEZdeAF4V7GS29i/HhYsQKWL890USIiqdPYIDjA3a9x9/ej6VpgaCoLy5gOPWDoN2H5n/jsuNWA+glEpH1rbBBsN7NjEw/MbBywPTUltQLDvwfVFQyz2+jdG2bMyHRBIiKp09gguBC41cyWm9ly4LfAd1JWVaZ1PQgKJ2PLbuPM07fz6KNQWprpokREUqOxZw0tcPdRwOHA4e4+GjgxpZVl2vBwgdlVU+9nxw64445MFyQikhpNukOZu2+OrjAG+H4K6mk9+h4PPcZQWHYTJ51Yza23QmVlposSEWl5+3KrSmuxKlqjmgvMFvOLi59jxQp4/PFMFyUi0vL2JQja/9n10QVmR+dfycgRm7nllkwXJCLS8vYYBGZWZmabG5jKgP3SVGPmZHeAo+/CNi/miSvP5J+zKpg3L9NFiYi0rD0Ggbt3cfeuDUxd3D0nXUVm1IDPw5F/oCjvOW4//2JuuaX9N4REJF7i8WW+rw74Fmz5gG/yS957ZChr115F376ZLkpEpGXsSx/BHplZvpm9YWYLzGyRmV3bwDbTzGydmc2Ppm+nqp59dvgv2NTjbP59ytW89qeHMl2NiEiLSVkQADuBE6PrD4qBiWZ2dAPbPezuxdHUes/WN6Pb5+9i4erjObnnNCo/eSXTFYmItIiUBUE0XPWW6GFuNLXtA+zZeawZ9hgfrB1C1cwvh3sXiIi0calsEWBm2WY2H1gLPO/urzew2elmttDMHjGzQbt5nwsSQ2CvW7culSXv1Ukn9+Sih59l67ZceOmLsO2TjNYjIrKvUhoE7l7l7sVAIXCkmR1Wb5MngSJ3Pxx4Hrh3N+9zu7uXuHtJnz59UlnyXmVlwVe+PoSJv3qSqm1r4OlD4N3fgVdntC4RkeZKaRAkuPtG4CVgYr31pe6+M3p4BzA2HfXsq2nTYMm6I/nxy3PDfY7fvBj+cQxsmJ/p0kREmiyVZw31MbPu0XJH4HPAknrbDEh6OBlYnKp6WlLXrvDNb8LNdw1n9SEvwGcegK0fwN/HwpzvQ0VZpksUEWm0VLYIBgAvmdlC4E1CH8FTZnadmU2Otrk0OrV0AXApMC2F9bSoSy4Jg9D9168NhpwDk5bAAefD0t+Ew0UfPQLVFZkuU0Rkr8zb2A15S0pKfPbs2ZkuA4ALLwzDU8+dC4cfHq1c909480LYuBByCqDPsdDvhDD1GA1ZuoZPRNLPzOa4e0mDzykImm/9ehg+HIYNg1dfDR3JAFRXwsq/wZoXYc1LsDk64pXbFfocBwMmwtBvhMciImmwpyBIS2dxe9WzJ/z61zBrFtybfL5TVg4MPh2OuBUmvQ1fWQXHPAT7T4Wyd2HOJfBYIcz9AWz9MGP1i4iAWgT7rLoaxo+HxYth6VLo1asRLyqdDUt+Ax89HB4PmhLufdD7yJTWKiLxpRZBCmVlwW23wcaNcPXVjXxRrxIY9yBM/iAEwKq/wz+OguePDdckrHkZtq+BNhbSItI2qUXQQq64Am68MRwmOrqhEZX2pKIM3r8bltwUTkNN6NADuo6onfL7QW4XyOkSOqJzu0SPC8CiTK//3zM7H3I67dPfJiJtnzqL06CsDEaMgH794I03IKc5Jwe5w7YVsHlJmDYtrl3esbr5xRUMhe6joEcx9BgVljvvH27HmS5VO2D7qrrTjmheuQWqy6GqPMy9IixTDYVfhoMuhg7d0lerSDukIEiTv/wFzjwTbrklXGfQoso3QXlpaD1UlEFl0rxya92WQPIXfMXmcCrrhgWhozox7l9u9xAQHbqFs5dyuoZ5Yuo8GLoeDF2H77lFsbMUNv4rTGXvQvkGKN8IFRvrziu37PpaywmtnA7dwHIhq0O4K1xiubIMPv1nqGfYv8HwyyA/s0OMiLRVCoI0cYeJE+H//g+WLIEBA/b+mrSq3Bq+sDcsgI0LYOtHIShqpk1h8qqkF1loPXQdEYKhYEg402njv2DTv8Iv+oScLpDXGzp0D1Nu0jyvJ3TcDzoOqJ3n9a49pLU76+fCol/BikfDYa4DL4ARP4DODY5PKCK7oSBIo3ffhcMOgylT4MEHM11NM7hD1TbY8kG4/mHTkjDfvDgMu121PXwhdz0Euh8G3UdCt5Fh3nFA6g43bVoCi6+HDx4InzHwVMjuGFoaiamiLDrMlBi+yurOzaDgANjv5DB1OzS9h8dEMkhBkGbXXAPXXRcOFU2ZkulqWpBXw441kNcXsrIzU8PWD2HxDbDir+HwUaKzPHnKzksuOpo5UA3r54XWEECnwbWh0P9EyOkM1VVQtRUqkgKmakf4nA49Qwd+dsfmBUhVedh/O1aHQPWqcPGhV4FX1rbEOu8fAiu3y77sKZE6FARptnMnTJgACxfCa69BcXGmK5I6tq2ET56FT56B1c+HQ2ZZuaHPomr73l+f1aE2FHK7hRZSdl6YZyUtV26LOsVXh2lnadPqzO8HXQ6EggOhy0EhIHK71e3LSUyWHQKrans0T17enjQlPa6ugE4DQ19RwQHhMJ60WwqCDFi9Go44IvxwfPPNcDaRtEJVO2Hdq7B6RvhFnmhV5Ca1MLLyoHJz6AjfuT7qEI/mFZvCe1TvDF+yiXnVjtBy6DgA8vvXm/cLrQ/LDlehW3Y05YQatn4YOt63LAvzsmWwPQ03QOrQIwqFodBxYFjn1VFLJZp7dag1u1P4G3IS885hXXV52CfliZMFNoUTBirKwr7M6w15vWrnHXqFz62zH5KmmrPNPkmaR8teGfZxTqcwz+4EOR3DslfV/nepLq9d9uqoD6tn9PlJ85zOYdua7ZOWq7aHQ6aVW8NUs7wtBH9yizTRSs3OD69Lbl0mtzLJCn1kiYms8IXhVQ2EeTQd9B045EfN+s+rIMiQuXPh2GNh9Gh48UXIy9v7a0QaVLk1tGQqNkdnjiV18leWhUNMOR2jFkl+9MWYvNyx9vnEl6XlhPfc8h5seb92XvZeOLUXS/pSzqpd9srwBVi5lT3efTanILRgOnQPy5VbQqto56fhPZojr3ftyQZZHUIdiS/pqu21jy0nOgstLwR5YtmyQkDtLA1n4TV1hODs/NrQSw6/5H6qmj6qJFl5dX9cZOfXHq70aEosW3bd/37J/00HToL9z2zWrttTEGgozBQaMyaMQXTmmWGk0rvuUt+kNFNO53Aqb0vL6wk9Dt/7dg1xD79SK7eGfpXKreELK7dbmHY30q57CK9EKJRvrO0jSZ6qq8KXd+JMs/z+4fTiluIeai5fH2qp2hYCIzk4sjpEy1F4NqZvrLqitrWQaDFl5bZc3SmgIEixM84IncfXXgsjR8L3v5/pikRaiFloXeR0BHo37XWJvo2CISkrr3F1RIcBOw9uuffNyq09hbqN0FhDafCzn8Hpp8MPfwjPPpvpakRE6lIQpEFWVjhEdPjhcNZZYaRSEZHWQkGQJp07w9/+Bvn5MGkSfPxxpisSEQkUBGk0eDA88QSsWxeuM1i5MtMViYgoCNLuqKPgH/+AtWtDGKxYkemKRCTuFAQZcPTRIQwSLYOPPsp0RSISZwqCDDnqKHj+eSgtDWHwoW5dLCIZoiDIoCOPhBdegA0bQhgsX57pikQkjhQEGVZSEsJg0yYYPx4++GDvrxERaUkKglZg7NgQBmVlMG4czJmT6YpEJE4UBK3EmDHwyiuQmwvHHx+uORARSQcFQSty2GHw+utw6KHwla/Ab35T91bEIiKpoCBoZfr3h5kz4bTTwgB1F18Mlc0csVdEpDEUBK1Qp07w5z/Dj34Ev/99GJJi8+ZMVyUi7ZWCoJXKyoL//E/44x9hxozQiazTS0UkFRQErdy3vx2Grl6xItz7+C9/yXRFItLepCwIzCzfzN4wswVmtsjMrm1gmzwze9jMlpnZ62ZWlKp62rLPfjbc9nLEiHC3s/PPh61bM12ViLQXqWwR7AROdPdRQDEw0cyOrrfNt4AN7n4g8Bvg+hTW06YNHQqvvgpXXQV33hkuRFuwINNViUh7kLIg8GBL9DA3muqfDHkqcG+0/Ahwkpnu6rs7ubnwH/8RxijatCkMUfE//6NTTEVk36S0j8DMss1sPrAWeN7dX6+3yUBgBYC7VwKbgF4NvM8FZjbbzGavW7culSW3CSedFFoDn/scXHopTJ4Mq1dnuioRaatSGgTuXuXuxUAhcKSZHdbM97nd3UvcvaRPnz4tW2Qb1acPPPkk3HxzGNL6kEPC7TDVOhCRpkrLWUPuvhF4CZhY76mPgUEAZpYDdANK01FTe2AWWgQLFoQgmDYNJk7UaaYi0jSpPGuoj5l1j5Y7Ap8DltTb7Ang3Gh5CvCiu37TNtWIEWGcot/+FmbNCkNV3HILVFVlujIRaQtS2SIYALxkZguBNwl9BE+Z2XVmNjna5k6gl5ktA74P/DiF9bRrWVnw3e/CW2/BccfB974X5m+/nenKRKS1s7b2A7ykpMRnz56d6TJaNQibHs4AAAwwSURBVHd48MEQBmVlYcyin/wECgoyXZmIZIqZzXH3koae05XF7ZAZfO1rsHhxmF9/PQwfDn/6kzqTRWRXCoJ2rG9fuOsu+Oc/YcAAOOeccEvMhQszXZmItCYKghg4+uhwn4M//AEWLYLRo+GSS8K9kkVEFAQxkZ0NF1wA77wDF10Et90GBx4IN90EO3dmujoRySQFQcz07BlOM507N9wr+fLLw+mnDz0E1dWZrk5EMkFBEFOjRoUrkp97Drp1g7PPDmMXvfhipisTkXRTEMTc5z8fWgf33Qfr1oVxjE4+GebNy3RlIpIuCgIhKwu+/nVYuhR+/etwltGYMfClL8Ebb2S6OhFJNQWB1MjPhyuugA8+gOuug9deg6OOCuMXvfZapqsTkVRREMguuneHn/4UPvww3Dd57lw49lg48UR46SVdlCbS3igIZLe6dIEf/Si0EG68MVypfOKJMG4cPPGEzjISaS8UBLJXnTuH00w/+ABuvRVWrYJTT4WRI0Mnc0VFpisUkX2hIJBGy8+Hiy+Gd98Ng9plZ8O558IBB4Rhr7duzXSFItIcCgJpspyccN3BggXw9NNQVBRGOh00KATFP/+pfgSRtkRBIM1mFq45eOWVcFbRxIlwzz1wzDEwbBhcey28916mqxSRvVEQSIs45pgwzPXq1XD33TB4cAiCAw8MncsPPKBWgkhrpSCQFtW1a7h38owZ4fTTX/0qjHL69a/D5z4XOpxFpHVREEjKDBoEP/5xuH3m734XrlI+7DC4+WbdT1mkNVEQSMplZcGFF4Z7IYwfD5ddBscfD0uWZLoyEQEFgaTRoEHhLKP77gsXpxUXh0NHug5BJLMUBJJWZqG/4O23YdIkuPpqGDIkzN95J9PVicSTgkAyon9/eOQReOqpcG+E66+H4cPDGUZ33AGbN2e6QpH4UBBIRp1ySjhctHJlCIP16+H880NQfO1r8NhjUFaW6SpF2jfzNnZyd0lJic+ePTvTZUiKuIezi+65B6ZPh40bITcXjjsuXLx28snh1ppmma5UpG0xsznuXtLgcwoCaa0qKmDWLHjmmTC99VZYX1QEX/hCuLXm6NFw6KHQoUNGSxVp9RQE0i589BE8+2yYXnyx9pBRbm64PmH06DCNGgUHHwy9e2e2XpHWREEg7U51dRjHaN68cOOcefPCtG5d7Ta9eoVAGDGidn7ggWH4i/z8zNUukgkKAokFd/j4Y/jXv8LFakuWhOsVliypGxAA/frB/vvXnQYPDtc6DBoUQkT9ENKe7CkIctJdjEiqmEFhYZi++MW6z5WWhkB4//0wBlJimj8/3G1t586623fsGN4nEQz1Q2PQIMjLS9/fJpJKCgKJhV69wjUK48bt+lx1NaxdCytW1J0++ijMX3gBPvlk19FTBwyAgQPDe/fsGaYePeouJ0/du0OnTmppSOujIJDYy8oK1y307w9HHNHwNuXl4VqHDz8MAZFoUXz8cRhd9b33wnzDhj3fyzk3N4RC374hSPr3r5337x8OWXXuHPowOnYM8+Tl3NzU7AOJt5QFgZkNAu4D+gEO3O7uN9fbZgLwNyAxOPFf3f26VNUk0lwdOsDQoWHak+rqcFV0aWkIhY0bawMiMa1fH1ogq1fD0qVhXl7euDpyckKrolOnEA6J5S5dalsiyVOPHmG77Ozw2vrzTp1C8CSm/Hy1WOIolS2CSuAH7j7XzLoAc8zseXd/u952r7r7pBTWIZI2WVnhEFD37o1/jXsIiNWrYc0a2L4dduyonSembdvCuu3bw3Li8dat4VTaRYtCyKxf3/yB/LKyakMhETKJ5eR5QUHDU4cOIWSyssI8eXIPQVldHYYhTyybQbduYUrsu86dFUjplLIgcPdVwKpouczMFgMDgfpBIBJrZrW/4A85ZN/fzz2EQyIUdu6Eysrw5Zs8r6gIYbJ1a8NTImwS0yefhPmWLbXhU1m57/U2JDs7BEOiT6WhKdGqSZ5yc2uXO3QIj3Nz6y4nT4nXJLbp2LHhKS9v189KTMlhlwjArKzwXOLQXmsPtbT0EZhZETAaeL2Bpz9jZguAT4Ar3H1RA6+/ALgAYPDgwakrVKQdMKv9hZ7qfy7l5SEYtmwJwVBeXvtrv6qq7pSVVTslviyzsmoPp23cWDtt2hTm27aFYGtoqq4OQZSYKipq5zt3hprKy8PjvU2pvlFScj9Px44heBKfnVxjeXn425IDLTngLroIrryy5etLeRCYWQHwKHCZu9cfU3IusL+7bzGzk4HHgYPqv4e73w7cDuE6ghSXLCKN1KFDbWumLXMPX8KJQ2+JQ3OJqby8btgkh08i6JLDLxFS9d8n8biiYteWSmLZrOGAq6wMpy6nQkqDwMxyCSHwoLv/tf7zycHg7s+Y2W1m1tvdP01lXSIiyczC4Z+8vKb177QXKRuG2swMuBNY7O437mab/tF2mNmRUT2lqapJRER2lcoWwTjg68C/zGx+tO5qYDCAu/8emAJcZGaVwHbgLG9rY16IiLRxqTxr6H+BPfaVu/tvgd+mqgYREdk73aFMRCTmFAQiIjGnIBARiTkFgYhIzCkIRERirs3doczM1gEfNvPlvQFdrFZL+6Mu7Y9a2hd1tYf9sb+792noiTYXBPvCzGbv7lZtcaT9UZf2Ry3ti7ra+/7QoSERkZhTEIiIxFzcguD2TBfQymh/1KX9UUv7oq52vT9i1UcgIiK7iluLQERE6lEQiIjEXGyCwMwmmtlSM1tmZj/OdD3pZmZ3mdlaM3sraV1PM3vezN6N5j0yWWO6mNkgM3vJzN42s0Vm9r1ofVz3R76ZvWFmC6L9cW20foiZvR79m3nYzDpkutZ0MbNsM5tnZk9Fj9v1vohFEJhZNnAr8EXgEGCqmbXAbcLblHuAifXW/RiY4e4HATOix3FQCfzA3Q8Bjga+G/3/ENf9sRM40d1HAcXARDM7Grge+I27HwhsAL6VwRrT7XvA4qTH7XpfxCIIgCOBZe7+vruXA9OBUzNcU1q5+yvA+nqrTwXujZbvBb6c1qIyxN1XufvcaLmM8A9+IPHdH+7uW6KHudHkwInAI9H62OwPMysETgHuiB4b7XxfxCUIBgIrkh6vjNbFXT93XxUtrwb6ZbKYTDCzImA08Dox3h/RoZD5wFrgeeA9YKO7V0abxOnfzE3AlUB19LgX7XxfxCUIZC+iW4TG6lxiMysAHgUuc/fNyc/FbX+4e5W7FwOFhBb0iAyXlBFmNglY6+5zMl1LOqXynsWtycfAoKTHhdG6uFtjZgPcfZWZDSD8GowFM8slhMCD7v7XaHVs90eCu280s5eAzwDdzSwn+iUcl38z44DJZnYykA90BW6mne+LuLQI3gQOinr+OwBnAU9kuKbW4Ang3Gj5XOBvGawlbaJjvncCi939xqSn4ro/+phZ92i5I/A5Qr/JS8CUaLNY7A93v8rdC929iPA98aK7n0M73xexubI4SvibgGzgLnf/ZYZLSiszewiYQBhOdw1wDfA48GdgMGFo7zPdvX6HcrtjZscCrwL/ovY48NWEfoI47o/DCR2g2YQfh3929+vMbCjhxIqewDzga+6+M3OVppeZTQCucPdJ7X1fxCYIRESkYXE5NCQiIruhIBARiTkFgYhIzCkIRERiTkEgIhJzCgKResysyszmJ00tNvicmRUljwAr0hrE5cpikabYHg23IBILahGINJKZLTez/zKzf0Xj9x8YrS8ysxfNbKGZzTCzwdH6fmb2WDTO/wIzOyZ6q2wz+2M09v8/oqt5RTJGQSCyq471Dg19Nem5Te4+Evgt4Up1gP8B7nX3w4EHgVui9bcAL0fj/I8BFkXrDwJudfdDgY3A6Sn+e0T2SFcWi9RjZlvcvaCB9csJN3B5Pxq0brW79zKzT4EB7l4RrV/l7r3NbB1QmDwUQTTs9fPRzW8wsx8Bue7+76n/y0QaphaBSNP4bpabInmMmirUVycZpiAQaZqvJs3/GS3PIoxUCXAOYUA7CLe7vAhqbvzSLV1FijSFfomI7KpjdLeuhL+7e+IU0h5mtpDwq35qtO4S4G4z+yGwDvhmtP57wO1m9i3CL/+LgFWItDLqIxBppKiPoMTdP810LSItSYeGRERiTi0CEZGYU4tARCTmFAQiIjGnIBARiTkFgYhIzCkIRERi7v8DyTc6e2WmtaAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCEX3ethL49",
        "colab_type": "text"
      },
      "source": [
        "## BLEU scores\n",
        "Generate BLEU-1 through BLEU-4 scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPcP26aKaXnD",
        "colab_type": "text"
      },
      "source": [
        "## Load best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8hiqXV_kyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find best model file name from model.fit output and insert below\n",
        "best_model = 'checkpoints/rebecca/final/model-ep036-loss2.504-val_loss3.272.h5' # CHANGE THIS\n",
        "\n",
        "# Load best model\n",
        "best_model = tf.keras.models.load_model(best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzPM-AfNSwqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict a caption for a word using model\n",
        "def predict_caption(model, photo, wordtoix, ixtoword, max_length):\n",
        "    \"\"\"\n",
        "    Inputs      - model = Tensorflow model object after training\n",
        "                - photo = image features vector\n",
        "                - wordtoix = word to index dictionary\n",
        "                - ixtoword = index to word dictionary\n",
        "                - max_length = max length of caption\n",
        "    Outputs     - final = predicted caption for image as string\n",
        "    \"\"\"\n",
        "    # Reshape feature vector for input into trained model\n",
        "    photo = photo.reshape((1,2048))\n",
        "\n",
        "    # Start prediction caption with startseq\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    # Predict caption using trained model\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    # Break caption into strings\n",
        "    final = in_text.split()\n",
        "\n",
        "    # Remove startseq\n",
        "    final = final[1:-1]\n",
        "\n",
        "    # Join caption into single string\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlytc6SphqTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, captions, features, wordix, ixword, max_length, dataset = 'val'):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- model = Tensorflow model object after training\n",
        "\t\t\t\t\t\t\t- captions = pre-processed caption dictionary\n",
        "\t\t\t\t\t\t\t- features = image feature vectors\n",
        "              - wordtoix = word to index dictionary\n",
        "              - ixtoword = index to word dictionary\n",
        "              - max_length = max length of caption\t\n",
        "\t\t\t\t\t\t\t- dataset = string, either 'val' or 'test'\t\t\t\t\t\t\n",
        "\tOutputs\t\t\t- none, prints results to screen\n",
        "\t\"\"\"\n",
        "\tactual, predicted = list(), list()\n",
        "\t# Step over the whole set\n",
        "\tfor key, desc_list in captions.items():\n",
        "\t\t\n",
        "\t\t# Generate description (fix because feature files were generated in different folders)\n",
        "\t\tif dataset == 'test':\n",
        "\t\t\ttemp = '/content/drive/Shared drives/DL_AT3/ImageCaptioning/Flicker8k_Dataset/'\n",
        "\t\telse:\n",
        "\t\t\ttemp = '/content/drive/Shared drives/DL_AT3/Experimentation/Flicker8k_Dataset/'\n",
        "\t\tphoto = features[temp + key + '.jpg']\n",
        "\t\tyhat = predict_caption(model, photo, wordix, ixword, max_length)\n",
        "\t\t\n",
        "\t\t# Store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t\n",
        "\t# Calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3_LcpXKaRJl",
        "colab_type": "text"
      },
      "source": [
        "### Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElfjpCj_A4Xa",
        "colab_type": "code",
        "outputId": "f66017b1-403c-4a35-e739-08f01128fccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Calculate BLEU scores on validation dataset for best model\n",
        "tic = time.perf_counter()\n",
        "print('Validation set:')\n",
        "evaluate_model(best_model, val_captions, val_features, word_to_index, index_to_word, max_length_caption)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model evaluated in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set:\n",
            "BLEU-1: 0.467540\n",
            "BLEU-2: 0.286222\n",
            "BLEU-3: 0.202820\n",
            "BLEU-4: 0.098692\n",
            "Model evaluated in: 5.12 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJrYZ5VwJ4pW",
        "colab_type": "text"
      },
      "source": [
        "### Test dataset\n",
        "Only run this for **THE BEST** models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edl-ySk7raWt",
        "colab_type": "code",
        "outputId": "40004bdb-fadd-47f8-bc7d-b2b2037a99e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Calculate BLEU scores on test dataset for best model\n",
        "tic = time.perf_counter()\n",
        "print('Test set:')\n",
        "evaluate_model(best_model, test_captions, test_features, word_to_index, index_to_word, max_length_caption, 'test')\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model evaluated in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set:\n",
            "BLEU-1: 0.458873\n",
            "BLEU-2: 0.287829\n",
            "BLEU-3: 0.211063\n",
            "BLEU-4: 0.110995\n",
            "Model evaluated in: 4.85 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF08Py8ca8Ht",
        "colab_type": "text"
      },
      "source": [
        "# Save files for submission\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1VjGB_CbAh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of data required for evaluation\n",
        "final_model_data = {}\n",
        "final_model_data['val_captions'] = val_captions\n",
        "final_model_data['val_features'] = val_features\n",
        "final_model_data['test_captions'] = test_captions\n",
        "final_model_data['test_features'] = test_features\n",
        "final_model_data['word_to_index'] = word_to_index\n",
        "final_model_data['index_to_word'] = index_to_word\n",
        "final_model_data['max_length_caption'] = max_length_caption\n",
        "\n",
        "# Save to pickle file for submission\n",
        "dump(final_model_data, open('final_model_data.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRiYih6Ia1xU",
        "colab_type": "text"
      },
      "source": [
        "# SUBMISSION: FINAL MODEL AND RESULTS\n",
        "\n",
        "Run cell to print out BLEU scores for validation and testing dataset\n",
        "\n",
        "Requires `final_model_data.pkl` and `final_model.h5` to be located in working directory\n",
        "\n",
        "May take 10-15 minutes to run. Output of BLEU scores for validation set and for testing set will be printed to screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL5UuLPPdUoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "ffd60c2c-53aa-4be0-fdce-1721e40589a8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import pickle\n",
        "from pickle import load\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)\n",
        "\n",
        "# Predict a caption for a word using model\n",
        "def predict_caption(model, photo, wordtoix, ixtoword, max_length):\n",
        "    \"\"\"\n",
        "    Inputs      - model = Tensorflow model object after training\n",
        "                - photo = image features vector\n",
        "                - wordtoix = word to index dictionary\n",
        "                - ixtoword = index to word dictionary\n",
        "                - max_length = max length of caption\n",
        "    Outputs     - final = predicted caption for image as string\n",
        "    \"\"\"\n",
        "    # Reshape feature vector for input into trained model\n",
        "    photo = photo.reshape((1,2048))\n",
        "\n",
        "    # Start prediction caption with startseq\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    # Predict caption using trained model\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    # Break caption into strings\n",
        "    final = in_text.split()\n",
        "\n",
        "    # Remove startseq\n",
        "    final = final[1:-1]\n",
        "\n",
        "    # Join caption into single string\n",
        "    final = ' '.join(final)\n",
        "    return final\n",
        "\n",
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, captions, features, wordix, ixword, max_length, dataset = 'val'):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- model = Tensorflow model object after training\n",
        "\t\t\t\t\t\t\t- captions = pre-processed caption dictionary\n",
        "\t\t\t\t\t\t\t- features = image feature vectors\n",
        "              - wordtoix = word to index dictionary\n",
        "              - ixtoword = index to word dictionary\n",
        "              - max_length = max length of caption\t\n",
        "\t\t\t\t\t\t\t- dataset = string, either 'val' or 'test'\t\t\t\t\t\t\n",
        "\tOutputs\t\t\t- none, prints results to screen\n",
        "\t\"\"\"\n",
        "\tactual, predicted = list(), list()\n",
        "\t# Step over the whole set\n",
        "\tfor key, desc_list in captions.items():\n",
        "\t\t\n",
        "\t\t# Generate description (fix because feature files were generated in different folders)\n",
        "\t\tif dataset == 'test':\n",
        "\t\t\ttemp = '/content/drive/Shared drives/DL_AT3/ImageCaptioning/Flicker8k_Dataset/'\n",
        "\t\telse:\n",
        "\t\t\ttemp = '/content/drive/Shared drives/DL_AT3/Experimentation/Flicker8k_Dataset/'\n",
        "\t\tphoto = features[temp + key + '.jpg']\n",
        "\t\tyhat = predict_caption(model, photo, wordix, ixword, max_length)\n",
        "\t\t\n",
        "\t\t# Store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t\n",
        "\t# Calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# Load final_model_data.pkl\n",
        "data = pickle.load(open('final_model_data.pkl', 'rb'))\n",
        "\n",
        "# Load final model\n",
        "final_model = tf.keras.models.load_model('final_model.h5')\n",
        "\n",
        "# Get validation BLEU scores\n",
        "print('Validation set:')\n",
        "evaluate_model(final_model, data['val_captions'], data['val_features'], \\\n",
        "               data['word_to_index'], data['index_to_word'], \\\n",
        "               data['max_length_caption'])\n",
        "print('------------')\n",
        "\n",
        "# Get test BLEU scores\n",
        "print('test set:')\n",
        "evaluate_model(final_model, data['test_captions'], data['test_features'], \\\n",
        "               data['word_to_index'], data['index_to_word'], \\\n",
        "               data['max_length_caption'] , dataset = 'test')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set:\n",
            "BLEU-1: 0.467540\n",
            "BLEU-2: 0.286222\n",
            "BLEU-3: 0.202820\n",
            "BLEU-4: 0.098692\n",
            "------------\n",
            "test set:\n",
            "BLEU-1: 0.458873\n",
            "BLEU-2: 0.287829\n",
            "BLEU-3: 0.211063\n",
            "BLEU-4: 0.110995\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}