{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CocoTheAussieCat/dl_at3/blob/master/Colab_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK7eXHIsc6t",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive Data Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBM_yUOsezf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzzNZH5tvKB",
        "colab_type": "code",
        "outputId": "f8cf981b-cfbb-4ce8-dade-1374261f89af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Direct the workbook into the project folder\n",
        "\n",
        "%cd drive/Shared\\ drives/DL_AT3/Cloud_Folder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Cloud_Folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X0FuZOkWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "dataset_dir = os.getcwd() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3_tIgRkfeL",
        "colab_type": "code",
        "outputId": "03c6ed86-e2cf-4857-85e8-7545b9117c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the directory\n",
        "dataset_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/Cloud_Folder'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a1biZwZGqt",
        "colab_type": "code",
        "outputId": "f268177a-5c7f-48ef-bb58-8784a2b9972d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Cloud_Folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGMBdWc0SVR",
        "colab_type": "code",
        "outputId": "747cdae4-e26f-427f-e66f-140f0635e226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#Check the files inside the directory\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab_Model.ipynb\t    Flickr8k_Dataset.zip      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr_8k.devImages.txt   __MACOSX\n",
            "descriptions.txt\t    Flickr8k.lemma.token.txt  model.png\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt  readme.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\t      saved_model\n",
            "Flicker8k_Dataset\t    Flickr8k.token.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32A5lyYpJZP",
        "colab_type": "text"
      },
      "source": [
        "## Unzip File from Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo61q99biV6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Image Dataset\n",
        "#!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUrNOy-UAGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Text Dataset\n",
        "#!unzip Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5wTgKJPjxaL",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7XZ57Sj4TC",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQY7v5tkDqA",
        "colab_type": "text"
      },
      "source": [
        "Import libraries, set working directory and relative paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzW9k4cj0oF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ff9e4ac-72a6-430f-d231-fa09f5e65dd2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Embedding, Dropout, Add\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\t\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NsXujecUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD1R001PnQDN",
        "colab_type": "code",
        "outputId": "b0f872a3-6bc0-408e-a408-a03c5894ba0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Check the folder's content after unziping \n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab_Model.ipynb\t    Flickr8k_Dataset.zip      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr_8k.devImages.txt   __MACOSX\n",
            "descriptions.txt\t    Flickr8k.lemma.token.txt  model.png\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt  readme.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\t      saved_model\n",
            "Flicker8k_Dataset\t    Flickr8k.token.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV41L_nfkYWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dataset directory and relative directories\n",
        "image_dir = dataset_dir + '/Flicker8k_Dataset'\n",
        "caption_dir = dataset_dir + '/Flickr8k.token.txt'\n",
        "train_dir = dataset_dir + '/Flickr_8k.trainImages.txt'\n",
        "test_dir = dataset_dir + '/Flickr_8k.testImages.txt'\n",
        "val_dir = dataset_dir + '/Flickr_8k.devImages.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBnb6t25n7dn",
        "colab_type": "text"
      },
      "source": [
        "# Prepare image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCw-CzHVn_kC",
        "colab_type": "text"
      },
      "source": [
        "Extract image features\n",
        "\n",
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Used VGG16 pre-trained model to extract image features by:\n",
        "\n",
        "Loading VGG16 pre-trained model.\n",
        "Removing top layer (because this layer is used for classification, which is not what is required)\n",
        "Extract features from each image by using predict function of VGG16 model.\n",
        "Create image_id by extracting the characters before .jpg in the file name.\n",
        "Store these features as vector of length 4096 in dictionary with image_id as key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRubioH2AWUe",
        "colab_type": "text"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "# Use VGG16 model, without top layer, add flatten and dense layer to get output of 4096\n",
        "# which is the required shape for LSTM model\n",
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "\tmodel_new = tf.keras.Sequential([\n",
        "  \tmodel,\n",
        "  \tFlatten(),\n",
        "\tDense(4096)])\n",
        "\t# remove top layer from model\n",
        "\t# model.layers.pop()\n",
        "\t# model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# print summary\n",
        "\tprint(model_new.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict() # create empty dictionary to store features in\n",
        "\tfor name in os.listdir(directory):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare image for VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model_new.predict(image, verbose=0)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature in dictionary using image_id as key\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\tprint('>%s' % name)\n",
        "\treturn features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKsiIStoK3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ONLY RUN IF YOU DON'T HAVE features.pkl IN YOUR ENVIRONMENT\n",
        "### TAKES >  1HOUR TO RUN\n",
        "# Extract features from all images\n",
        "#features = extract_features(image_dir)\n",
        "#print('Extracted Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJ_UxHooeAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save feature as pickle file\n",
        "#dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrZQgR9oYpZ",
        "colab_type": "text"
      },
      "source": [
        "# Prepare text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0HSuKMtzEpp",
        "colab_type": "text"
      },
      "source": [
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Get cleaned caption for each image by:\n",
        "\n",
        "Loading captions from text file.\n",
        "Creating dictionary of captions using image_id as key.\n",
        "Clean all captions by removing digits, single letter words (eg: a), punctuation and converting to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qouDIcpC67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and read image description file\n",
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tReads all captions from txt file as single string\n",
        "\tInputs\t\t- filename = filename of .txt with image captions\n",
        "\tOutputs\t\t- text = string\n",
        "\t\"\"\"\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUlitqApC69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\t\"\"\"\n",
        "    Inputs      - doc = string, output from load_doc()\n",
        "    Outputs     - mapping = dictionary-list of image_id and captions \n",
        "    \"\"\"\n",
        "\tcaption_dict = {}\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in caption_dict:\n",
        "\t\t\tcaption_dict[image_id] = list()\n",
        "\t\t\t# store description\n",
        "\t\t\tcaption_dict [image_id].append(image_desc)\n",
        "\treturn caption_dict "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_b_6DE3pC7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsvkrzxpC7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert descriptions into vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp40MuApC7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save descriptions to file, one image_id and description per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYzk-jmpC7G",
        "colab_type": "code",
        "outputId": "28ee71fa-b36a-42cf-a13b-dbf20de2bbb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load descriptions from tokenised text file\n",
        "doc = load_doc(caption_dir)\n",
        "\n",
        "# Create dictionary of image_id and descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Clean descriptions by stripping digits, punctuation, single letter words and converting to lowercase\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Create vocab from descriptions and get vocab length\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Save descriptions to file, one image_id and description per line\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 4473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FToIspC7J",
        "colab_type": "text"
      },
      "source": [
        "# Load pre-processed training data for modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6F1vMYpC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "\t# open file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyGLW9mZpC7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-FH_B4UpC7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMH5gzMpC7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t# load all features\n",
        "\tall_features = pickle.load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrtTPd0IpC7S",
        "colab_type": "text"
      },
      "source": [
        "## Tokenise descriptions\n",
        "Map unique words to integers using tf.keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4QHBL5opC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert dictionary of clean descriptions to list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Bh83aHpC7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0pKaW9Sn2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv8ETrBVpC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to calculate length of description with most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDMyp13LpC7f",
        "colab_type": "text"
      },
      "source": [
        "## Load train and validation data for modelling\n",
        "Images loaded as numpy arrays, descriptions tokenised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zk8KGNhAvan",
        "colab_type": "code",
        "outputId": "123bf22b-a70b-494a-e8ff-d600d2a48c7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Load training set\n",
        "train = load_set(train_dir)\n",
        "print('Dataset: %d' % len(train))\n",
        "\n",
        "# Load training set descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train = %d' % len(train_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train = %d' % len(train_features))\n",
        "\n",
        "# Prepare sequences of descriptions for train, test and validation sets\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# Determine max sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train = 6000\n",
            "Photos: train = 6000\n",
            "Vocabulary Size: 3848\n",
            "Description Length: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSEqSUTEA5rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKQ-7sGQpC7j",
        "colab_type": "code",
        "outputId": "3ec95945-94b6-44c5-ea08-52159b69d124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Load validation set (using devImages)\n",
        "val = load_set(val_dir)\n",
        "print('Dataset: %d' % len(val))\n",
        "\n",
        "# Load training set descriptions\n",
        "val_descriptions = load_clean_descriptions('descriptions.txt', val)\n",
        "print('Descriptions: val = %d' % len(val_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "val_features = load_photo_features('features.pkl', val)\n",
        "print('Photos: val = %d' % len(val_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: val = 1000\n",
            "Photos: val = 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMvzjme4upR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1val, X2val, yval = create_sequences(tokenizer, max_length, val_descriptions, val_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt1mIMIopC7n",
        "colab_type": "text"
      },
      "source": [
        "# Define model\n",
        "Based on merge-model described by Tanti et al. in *Where to put the Image in an Image Caption Generator*\n",
        "\n",
        "source: <https://arxiv.org/abs/1703.09137>\n",
        "\n",
        "code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZllMSBpC7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = Add()([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summary\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWe-aYjpC7q",
        "colab_type": "text"
      },
      "source": [
        "# Train model\n",
        "Use checkpoint callbacks to save training informatoin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi12TajEpC7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define where to save checkpoints\n",
        "!mkdir -p checkpoints\n",
        "check_dir = dataset_dir + '/checkpoints/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\n",
        "# Monitor validation loss, saving only the best\n",
        "checkpoint = ModelCheckpoint(check_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngMLZQdpC7t",
        "colab_type": "code",
        "outputId": "5610d270-b5d7-436c-efed-f1a6131035ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# Create base model\n",
        "base_model = define_model(vocab_size, max_length)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 256)      985088      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 30, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3848)         988936      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,613,960\n",
            "Trainable params: 3,613,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfje5knBVYR",
        "colab_type": "code",
        "outputId": "f21e7f53-64d2-4ecb-b910-775fa71fcf72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### DON'T RUN THIS LOCALLY ON YOUR LAPTOP! PROBABLY WON'T HAVE ENOUGH RAM\n",
        "# Fit model\n",
        "tic = time.perf_counter()\n",
        "base_history = base_model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, validation_data=([X1val, X2val], yval), callbacks = [checkpoint])\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.4f} seconds')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.31536, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep001-loss5.926-val_loss5.315.h5\n",
            "1916/1916 - 147s - loss: 5.9261 - val_loss: 5.3154\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.31536 to 5.17150, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep002-loss5.375-val_loss5.172.h5\n",
            "1916/1916 - 145s - loss: 5.3750 - val_loss: 5.1715\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.17150 to 5.01312, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep003-loss5.184-val_loss5.013.h5\n",
            "1916/1916 - 145s - loss: 5.1840 - val_loss: 5.0131\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.01312 to 4.92314, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep004-loss5.056-val_loss4.923.h5\n",
            "1916/1916 - 145s - loss: 5.0561 - val_loss: 4.9231\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 4.92314 to 4.86558, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep005-loss4.902-val_loss4.866.h5\n",
            "1916/1916 - 144s - loss: 4.9020 - val_loss: 4.8656\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 4.86558 to 4.82756, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep006-loss4.809-val_loss4.828.h5\n",
            "1916/1916 - 144s - loss: 4.8092 - val_loss: 4.8276\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 4.82756 to 4.81582, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep007-loss4.724-val_loss4.816.h5\n",
            "1916/1916 - 144s - loss: 4.7239 - val_loss: 4.8158\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss improved from 4.81582 to 4.76715, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep008-loss4.646-val_loss4.767.h5\n",
            "1916/1916 - 144s - loss: 4.6457 - val_loss: 4.7671\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss improved from 4.76715 to 4.76136, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep009-loss4.557-val_loss4.761.h5\n",
            "1916/1916 - 142s - loss: 4.5568 - val_loss: 4.7614\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss improved from 4.76136 to 4.70656, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep010-loss4.468-val_loss4.707.h5\n",
            "1916/1916 - 142s - loss: 4.4676 - val_loss: 4.7066\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 4.70656 to 4.70574, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep011-loss4.396-val_loss4.706.h5\n",
            "1916/1916 - 142s - loss: 4.3956 - val_loss: 4.7057\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 4.70574 to 4.69918, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep012-loss4.310-val_loss4.699.h5\n",
            "1916/1916 - 141s - loss: 4.3099 - val_loss: 4.6992\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 4.69918 to 4.67014, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep013-loss4.212-val_loss4.670.h5\n",
            "1916/1916 - 141s - loss: 4.2119 - val_loss: 4.6701\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 4.67014 to 4.61236, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep014-loss4.097-val_loss4.612.h5\n",
            "1916/1916 - 142s - loss: 4.0972 - val_loss: 4.6124\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss improved from 4.61236 to 4.57771, saving model to /content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep015-loss4.187-val_loss4.578.h5\n",
            "1916/1916 - 142s - loss: 4.1869 - val_loss: 4.5777\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 4.57771\n",
            "1916/1916 - 142s - loss: 4.0084 - val_loss: 4.6008\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 4.57771\n",
            "1916/1916 - 141s - loss: 3.8386 - val_loss: 4.6102\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 4.57771\n",
            "1916/1916 - 141s - loss: 3.7415 - val_loss: 4.6497\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 4.57771\n",
            "1916/1916 - 140s - loss: 3.7728 - val_loss: 4.6575\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 4.57771\n",
            "1916/1916 - 140s - loss: 3.6956 - val_loss: 4.6986\n",
            "Model ran in 2858.0019 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqfGqkDpAVIV",
        "colab_type": "code",
        "outputId": "96349e06-688d-4347-9524-67253f5c7929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "check_dir"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/Cloud_Folder/checkpoints/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72_faJBlGxSq",
        "colab_type": "code",
        "outputId": "36b0c120-6323-47d1-cb07-5d341bfc5666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the model has been saved\n",
        "# my_model directory\n",
        "!ls checkpoints"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'checkpoints': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glEPDekAAAgS",
        "colab_type": "code",
        "outputId": "b283f7a0-f99b-417d-e7de-683463e648cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab_Model.ipynb\t    Flickr8k_Dataset.zip      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr_8k.devImages.txt   __MACOSX\n",
            "descriptions.txt\t    Flickr8k.lemma.token.txt  model.png\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt  readme.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\n",
            "Flicker8k_Dataset\t    Flickr8k.token.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8fCZMbJADOW",
        "colab_type": "code",
        "outputId": "1db16341-52ac-4c01-8554-05d2c921ba14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# SAVE MODEL IN CASE CHECKPOINT DIDNT WORK\n",
        "!mkdir -p saved_model\n",
        "base_model_dir = 'saved_model/base_model'\n",
        "base_model.save(base_model_dir) "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: saved_model/base_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjJqjlroDF1v",
        "colab_type": "code",
        "outputId": "bca97d19-4437-4630-f666-ebb802384e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Check the model has been saved\n",
        "# my_model directory\n",
        "!ls saved_model\n",
        "\n",
        "# Contains an assets folder, saved_model.pb, and variables folder.\n",
        "!ls saved_model/base_model"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "base_model\n",
            "assets\tsaved_model.pb\tvariables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCEX3ethL49",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-KaNKutipM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model\n",
        "base_model_reload = tf.keras.models.load_model(base_model_dir, compile = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIQ6K9t1wxgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "3a653ba5-e7e7-47a5-b42a-cc2a18299e94"
      },
      "source": [
        "base_model_reload.summary()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 256)      985088      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 30, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3848)         988936      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,613,960\n",
            "Trainable params: 3,613,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qk3hHaDhOGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse vectorisation of words to map integers back to words\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVv_Kz5hejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over whole length of sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlytc6SphqTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9avchiHWxduS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_descriptions\n",
        "val_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp3Q1MFzlB3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "845b994f-98f9-4d68-d3ca-b525f099242b"
      },
      "source": [
        "tic = time.perf_counter()\n",
        "evaluate_model(base_model_reload, val_descriptions, val_features, tokenizer, max_length)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.4f} seconds')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.295975\n",
            "BLEU-2: 0.092577\n",
            "BLEU-3: 0.054726\n",
            "BLEU-4: 0.014931\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}