{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Base_Model_for_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CocoTheAussieCat/dl_at3/blob/master/Base_Model_for_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_8s1MYpNpuh",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning - AT3 - Base Model\n",
        "Team name: AROCworkOrange\n",
        "\n",
        "Please run final cell for validation of results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eos88K9uKSNH",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "This notebook is based on code from Jason Brownlee's *How to Develop a Deep Learning Photo Caption Generator from Scratch*, 27 June 2019 (last updated 20 April 2020),  <https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/>.\n",
        "\n",
        "The code for data pre-processing, model architecture and evaluation with BLEU scores has been adapted from Brownlee's original work. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK7eXHIsc6t",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive Data Source -- RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBM_yUOsezf",
        "colab_type": "code",
        "outputId": "28d5a32c-222f-4cff-e6ad-079b796648a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0g-N2134K2x",
        "colab_type": "text"
      },
      "source": [
        "## Set directory\n",
        "This should be the base directory containing all the Flickr8k files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzzNZH5tvKB",
        "colab_type": "code",
        "outputId": "a75d2ef1-1cb8-46b7-ce83-b6fb027db4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Direct the workbook into the project folder\n",
        "\n",
        "%cd drive/Shared\\ drives/DL_AT3/Cloud_Folder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Cloud_Folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X0FuZOkWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "dataset_dir = os.getcwd() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3_tIgRkfeL",
        "colab_type": "code",
        "outputId": "b082c4c6-72b4-4b44-91fa-ed47342b86ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the directory\n",
        "dataset_dir"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/Cloud_Folder'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a1biZwZGqt",
        "colab_type": "code",
        "outputId": "e2943f63-78a2-48bb-cc80-89d7bfe36d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Cloud_Folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGMBdWc0SVR",
        "colab_type": "code",
        "outputId": "73047907-9587-4414-e6d9-201076e73ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#Check the files inside the directory\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base_Model_for_submission.ipynb  Flickr_8k.devImages.txt\n",
            "checkpoints\t\t\t Flickr8k.lemma.token.txt\n",
            "Colab_Model.ipynb\t\t Flickr_8k.testImages.txt\n",
            "CrowdFlowerAnnotations.txt\t Flickr8k_text.zip\n",
            "descriptions.txt\t\t Flickr8k.token.txt\n",
            "ExpertAnnotations.txt\t\t Flickr_8k.trainImages.txt\n",
            "features.pkl\t\t\t __MACOSX\n",
            "Flicker8k_Dataset\t\t model.png\n",
            "Flickr8k_Dataset.zip\t\t readme.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32A5lyYpJZP",
        "colab_type": "text"
      },
      "source": [
        "## Unzip files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo61q99biV6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Image Dataset\n",
        "#!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUrNOy-UAGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Text Dataset\n",
        "#!unzip Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cA9BBF43mUj",
        "colab_type": "text"
      },
      "source": [
        "## Set directories\n",
        "Relative paths based on `dataset_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV41L_nfkYWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dataset directory and relative directories\n",
        "image_dir = dataset_dir + '/Flicker8k_Dataset'\n",
        "caption_dir = dataset_dir + '/Flickr8k.token.txt'\n",
        "train_dir = dataset_dir + '/Flickr_8k.trainImages.txt'\n",
        "test_dir = dataset_dir + '/Flickr_8k.testImages.txt'\n",
        "val_dir = dataset_dir + '/Flickr_8k.devImages.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7XZ57Sj4TC",
        "colab_type": "text"
      },
      "source": [
        "# Setup -- RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQY7v5tkDqA",
        "colab_type": "text"
      },
      "source": [
        "Import libraries, set seeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kstP5OvJ4kL5",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzW9k4cj0oF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "110447ec-e6bf-4ec4-a13f-afb372d0e4aa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Embedding, Dropout, Add\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\t\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zb-WQiY3ukX",
        "colab_type": "text"
      },
      "source": [
        "## Set seeds\n",
        "Set seeds for reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NsXujecUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBnb6t25n7dn",
        "colab_type": "text"
      },
      "source": [
        "# Image pre-processing -- DO NOT RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCw-CzHVn_kC",
        "colab_type": "text"
      },
      "source": [
        "Extract image features\n",
        "\n",
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Used VGG16 pre-trained model to extract image features by:\n",
        "\n",
        "* Loading VGG16 pre-trained model.\n",
        "\n",
        "* Removing top layer (because this layer is used for classification, which is not what is required)\n",
        "\n",
        "* Extract features from each image by using predict function of VGG16 model.\n",
        "\n",
        "* Create image_id by extracting the characters before .jpg in the file name.\n",
        "\n",
        "* Store these features as vector of length 4096 in dictionary with image_id as key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQPOtkRt5MoH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgiCs_bp5DJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "model = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        " \n",
        "model_new = tf.keras.Sequential([\n",
        "  model,\n",
        "  Flatten(),\n",
        "Dense(4096)])\n",
        "# remove top layer from model\n",
        "# model.layers.pop()\n",
        "# model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "# print summary\n",
        "print(model_new.summary())\n",
        "# extract features from each photo\n",
        "features = dict() # create empty dictionary to store features in\n",
        "for name in os.listdir(directory):\n",
        "    # load an image from file\n",
        "    filename = directory + '/' + name\n",
        "    image = load_img(filename, target_size=(224, 224))\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # prepare image for VGG model\n",
        "    image = preprocess_input(image)\n",
        "    # get features\n",
        "    feature = model_new.predict(image, verbose=0)\n",
        "    # get image id\n",
        "    image_id = name.split('.')[0]\n",
        "    # store feature in dictionary using image_id as key\n",
        "    features[image_id] = feature\n",
        "    print('>%s' % name)\n",
        "return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKsiIStoK3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ONLY RUN IF YOU DON'T HAVE features.pkl IN YOUR ENVIRONMENT\n",
        "## TAKES >  1HOUR TO RUN\n",
        "Extract features from all images\n",
        "features = extract_features(image_dir)\n",
        "print('Extracted Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJ_UxHooeAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save feature as pickle file\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrZQgR9oYpZ",
        "colab_type": "text"
      },
      "source": [
        "# Caption pre-processing -- DO NOT RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0HSuKMtzEpp",
        "colab_type": "text"
      },
      "source": [
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Get cleaned caption for each image by:\n",
        "\n",
        "* Loading captions from text file.\n",
        "\n",
        "* Creating dictionary of captions using image_id as key.\n",
        "\n",
        "* Clean all captions by removing digits, single letter words (eg: a), punctuation and converting to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qouDIcpC67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and read image description file\n",
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tReads all captions from txt file as single string\n",
        "\tInputs\t\t- filename = filename of .txt with image captions\n",
        "\tOutputs\t\t- text = string\n",
        "\t\"\"\"\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUlitqApC69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\t\"\"\"\n",
        "  Inputs      - doc = string, output from load_doc()\n",
        "  Outputs     - mapping = dictionary-list of image_id and captions \n",
        "  \"\"\"\n",
        "\tcaption_dict = {}\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in caption_dict:\n",
        "\t\t\tcaption_dict[image_id] = list()\n",
        "\t\t\t# store description\n",
        "\t\t\tcaption_dict [image_id].append(image_desc)\n",
        "\treturn caption_dict "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_b_6DE3pC7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t\"\"\"\n",
        "  Inputs      - descriptions = raw descriptions\n",
        "  Outputs     - descriptions = cleaned descriptions\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsvkrzxpC7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert descriptions into vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t\"\"\"\"\n",
        "\tInputs\t\t- descriptions = cleaned descriptions\n",
        "\tOutputs\t\t- all_desc = set containing only unique words from descriptions\n",
        "\n",
        "\t\"\"\"\"\n",
        "\t# build list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp40MuApC7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save descriptions to file, one image_id and description per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t- descriptions = cleaned descriptions\n",
        "\t\t\t\t\t\t- filename = string, name to saves file to\n",
        "\tOutputs\t\t- saved file\n",
        "\t\"\"\"\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDhFWhjm6tOK",
        "colab_type": "text"
      },
      "source": [
        "## Create descriptions, save to .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYzk-jmpC7G",
        "colab_type": "code",
        "outputId": "467f7f74-5fbd-4d4f-b618-f6999456238f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load descriptions from tokenised text file\n",
        "doc = load_doc(caption_dir)\n",
        "\n",
        "# Create dictionary of image_id and descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Clean descriptions by stripping digits, punctuation, single letter words and converting to lowercase\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Create vocab from descriptions and get vocab length\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Save descriptions to file, one image_id and description per line\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 4473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FToIspC7J",
        "colab_type": "text"
      },
      "source": [
        "# Load data for modelling -- RUN\n",
        "Load pre-processed image features and descriptions in correct format to input into `model.fit()`.\n",
        "\n",
        "Note that `features.pkl` and `descriptions.txt` must be in the working directory folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6F1vMYpC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t- filename = .txt file containing cleaned descriptions\n",
        "\tOutputs\t\t- text\n",
        "\t\"\"\"\n",
        "\t# open file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyGLW9mZpC7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t- filename = .txt file containing cleaned descriptions\n",
        "\tOutputs\t\t- dataset = descriptions with image ids as set\n",
        "\t\"\"\"\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-FH_B4UpC7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t- filename = .txt file containing cleaned descriptions\n",
        "\tOutputs\t\t- dataset = descriptions dictionary with image id as key, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tdescriptions parsed with startseq and endseq\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsuitable for input to model.fit()\n",
        "\t\"\"\"\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMH5gzMpC7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t- filename = .pkl file containing feature vectors for images\n",
        "\t\t\t\t\t\t- dataset = string, train, val or test\n",
        "\tOutputs\t\t- features = image features\n",
        "\t\"\"\"\n",
        "\t# load all features\n",
        "\tall_features = pickle.load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrtTPd0IpC7S",
        "colab_type": "text"
      },
      "source": [
        "## Tokenise descriptions\n",
        "Map unique words to integers using tf.keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4QHBL5opC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert dictionary of clean descriptions to list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t- descriptions = dictionary of descriptions\n",
        "\tOutputs\t\t- all_desc = list of descriptions\n",
        "\t\"\"\"\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Bh83aHpC7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t\t- list of descriptions\n",
        "\tOutputs\t\t\t- tokenizer = tokens as tokenizer object\n",
        "\t\"\"\"\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv8ETrBVpC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to calculate length of description with most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfd8IRiW_TEl",
        "colab_type": "text"
      },
      "source": [
        "## Create description word sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0pKaW9Sn2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- tokenizer = output from create_tokenizer()\n",
        "\t\t\t\t\t\t\t- max_length = output from max_length()\n",
        "\t\t\t\t\t\t\t- descriptions = output from load_clean_descriptions()\n",
        "\t\t\t\t\t\t\t- photos = output from load_photo_features()\n",
        "\t\t\t\t\t\t\t- vocab_size = integer\n",
        "\tOutputs\t\t\t- X1 = numpy array of image features\n",
        "\t\t\t\t\t\t\t- X2 = numpy array of description sequences\n",
        "\t\t\t\t\t\t\t- y = numpy array of next word in description sequences\n",
        "\t\"\"\"\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDMyp13LpC7f",
        "colab_type": "text"
      },
      "source": [
        "## Load train and validation data for modelling\n",
        "Images loaded as numpy arrays, descriptions tokenised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYqnnGur_efN",
        "colab_type": "text"
      },
      "source": [
        "### Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zk8KGNhAvan",
        "colab_type": "code",
        "outputId": "17c33a09-8bb7-4c85-e775-6f7df038d442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Load training set\n",
        "train = load_set(train_dir)\n",
        "print('Dataset: %d' % len(train))\n",
        "\n",
        "# Load training set descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train = %d' % len(train_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train = %d' % len(train_features))\n",
        "\n",
        "# Prepare sequences of descriptions for train, test and validation sets\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# Determine max sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train = 6000\n",
            "Photos: train = 6000\n",
            "Vocabulary Size: 3848\n",
            "Description Length: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSEqSUTEA5rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn_AZxO9_hTQ",
        "colab_type": "text"
      },
      "source": [
        "### Validation data\n",
        "Note this is called devImages in Flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKQ-7sGQpC7j",
        "colab_type": "code",
        "outputId": "50bf775e-a18a-43d2-f9a0-11e58f4660c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Load validation set (using devImages)\n",
        "val = load_set(val_dir)\n",
        "print('Dataset: %d' % len(val))\n",
        "\n",
        "# Load training set descriptions\n",
        "val_descriptions = load_clean_descriptions('descriptions.txt', val)\n",
        "print('Descriptions: val = %d' % len(val_descriptions))\n",
        "\n",
        "# Extract validation set image features from features.pkl\n",
        "val_features = load_photo_features('features.pkl', val)\n",
        "print('Photos: val = %d' % len(val_features))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: val = 1000\n",
            "Photos: val = 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMvzjme4upR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1val, X2val, yval = create_sequences(tokenizer, max_length, val_descriptions, val_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyQTrify_sY0",
        "colab_type": "text"
      },
      "source": [
        "### Testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfHBNzRB_m-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7139a2e9-a78d-4d56-9cd4-1a0d30422136"
      },
      "source": [
        "# Load validation set (using devImages)\n",
        "test = load_set(test_dir)\n",
        "print('Dataset: %d' % len(test))\n",
        "\n",
        "# Load training set descriptions\n",
        "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test = %d' % len(test_descriptions))\n",
        "\n",
        "# Extract testing set image features from features.pkl\n",
        "test_features = load_photo_features('features.pkl', test)\n",
        "print('Photos: test = %d' % len(test_features))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test = 1000\n",
            "Photos: test = 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMDtDOopJJFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for evaluation of trained model\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt1mIMIopC7n",
        "colab_type": "text"
      },
      "source": [
        "# Image caption model -- RUN\n",
        "Based on merge-model described by Tanti et al. in *Where to put the Image in an Image Caption Generator*\n",
        "\n",
        "source: <https://arxiv.org/abs/1703.09137>\n",
        "\n",
        "code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZllMSBpC7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t\t- vocab_size = integer\n",
        "\tOutputs\t\t\t- model = Tensorflow model object\n",
        "\t\"\"\" \n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = Add()([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summary\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWe-aYjpC7q",
        "colab_type": "text"
      },
      "source": [
        "# Train model -- RUN\n",
        "Use checkpoint callbacks to save training information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi12TajEpC7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define where to save checkpoints\n",
        "!mkdir -p checkpoints/base\n",
        "check_dir = 'checkpoints/base/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\n",
        "# Monitor validation loss, saving only the best\n",
        "checkpoint = ModelCheckpoint(check_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngMLZQdpC7t",
        "colab_type": "code",
        "outputId": "658ac921-9255-4251-c741-ddab6c77d2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# Create base model\n",
        "base_model = define_model(vocab_size, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 256)      985088      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 30, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3848)         988936      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,613,960\n",
            "Trainable params: 3,613,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfje5knBVYR",
        "colab_type": "code",
        "outputId": "27440284-6875-43b1-c8fb-acdafe676b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit model\n",
        "tic = time.perf_counter()\n",
        "base_history = base_model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, validation_data=([X1val, X2val], yval), callbacks = [checkpoint, early_stopping, reduce_lr])\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.32062, saving model to checkpoints/base/model-ep001-loss5.931-val_loss5.321.h5\n",
            "1916/1916 - 148s - loss: 5.9306 - val_loss: 5.3206 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.32062 to 5.18643, saving model to checkpoints/base/model-ep002-loss5.376-val_loss5.186.h5\n",
            "1916/1916 - 148s - loss: 5.3760 - val_loss: 5.1864 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.18643 to 5.01077, saving model to checkpoints/base/model-ep003-loss5.175-val_loss5.011.h5\n",
            "1916/1916 - 149s - loss: 5.1753 - val_loss: 5.0108 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.01077 to 4.97305, saving model to checkpoints/base/model-ep004-loss5.065-val_loss4.973.h5\n",
            "1916/1916 - 148s - loss: 5.0647 - val_loss: 4.9730 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 4.97305 to 4.95634, saving model to checkpoints/base/model-ep005-loss4.975-val_loss4.956.h5\n",
            "1916/1916 - 147s - loss: 4.9752 - val_loss: 4.9563 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 4.95634 to 4.93606, saving model to checkpoints/base/model-ep006-loss4.926-val_loss4.936.h5\n",
            "1916/1916 - 148s - loss: 4.9264 - val_loss: 4.9361 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 4.93606 to 4.93053, saving model to checkpoints/base/model-ep007-loss4.864-val_loss4.931.h5\n",
            "1916/1916 - 148s - loss: 4.8637 - val_loss: 4.9305 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 4.93053\n",
            "1916/1916 - 152s - loss: 4.8148 - val_loss: 4.9310 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 4.93053\n",
            "1916/1916 - 151s - loss: 4.7893 - val_loss: 4.9769 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 4.93053\n",
            "1916/1916 - 147s - loss: 4.7419 - val_loss: 4.9488 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 4.93053 to 4.84460, saving model to checkpoints/base/model-ep011-loss4.719-val_loss4.845.h5\n",
            "1916/1916 - 146s - loss: 4.7193 - val_loss: 4.8446 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 4.84460 to 4.65661, saving model to checkpoints/base/model-ep012-loss4.473-val_loss4.657.h5\n",
            "1916/1916 - 147s - loss: 4.4730 - val_loss: 4.6566 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 4.65661 to 4.57457, saving model to checkpoints/base/model-ep013-loss4.282-val_loss4.575.h5\n",
            "1916/1916 - 146s - loss: 4.2819 - val_loss: 4.5746 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 4.57457 to 4.56728, saving model to checkpoints/base/model-ep014-loss4.157-val_loss4.567.h5\n",
            "1916/1916 - 147s - loss: 4.1565 - val_loss: 4.5673 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 4.56728\n",
            "1916/1916 - 148s - loss: 4.0336 - val_loss: 4.5727 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss improved from 4.56728 to 4.55430, saving model to checkpoints/base/model-ep016-loss3.955-val_loss4.554.h5\n",
            "1916/1916 - 148s - loss: 3.9554 - val_loss: 4.5543 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 4.55430\n",
            "1916/1916 - 146s - loss: 3.9056 - val_loss: 4.5847 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 4.55430\n",
            "1916/1916 - 145s - loss: 3.7327 - val_loss: 4.6080 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 4.55430\n",
            "1916/1916 - 145s - loss: 3.7334 - val_loss: 4.6778 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 4.55430\n",
            "1916/1916 - 144s - loss: 3.7885 - val_loss: 4.7283 - lr: 0.0010\n",
            "Model ran in: 49.24 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCEX3ethL49",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model -- RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGT0OFbmAh4Q",
        "colab_type": "text"
      },
      "source": [
        "## Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxiYw4JuAaV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelPlot(history_name, plt_title):\n",
        "    \"\"\"\n",
        "    Generates plot (with title) for model training and validation accuracy and loss by epoch\n",
        "    Inputs  - history_name = history object resulting from fitted model\n",
        "            - plt_title = title for plot, as string\n",
        "    Output  - plot\n",
        "    \"\"\"\n",
        "    from matplotlib.pyplot import savefig\n",
        "    loss_filename = check_path+'/loss.png'\n",
        "\n",
        "    plt.plot(history_name.history['loss'], label='Training', color = 'blue')\n",
        "    plt.plot(history_name.history['val_loss'], label='Validation', color = 'orange')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.title(plt_title)\n",
        "    plt.legend()\n",
        "    plt.savefig(loss_filename)\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9mk-SFJA1bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change title\n",
        "title = 'Base Model' # CHANGE THIS\n",
        "\n",
        "# Plot will save to check_path as loss.png\n",
        "modelPlot(history, title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrqLDnlYA7_q",
        "colab_type": "text"
      },
      "source": [
        "## BLEU scores\n",
        "Generate BLEU-1 through BLEU-4 scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9horpvHaJgSH",
        "colab_type": "text"
      },
      "source": [
        "### Load best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ZSLA0-A-5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find best model file name from model.fit output and insert below\n",
        "best_model = 'checkpoints/base/model-ep016-loss3.955-val_loss4.554.h5' # CHANGE THIS\n",
        "\n",
        "# Load best model\n",
        "best_model = tf.keras.models.load_model(best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qk3hHaDhOGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse vectorisation of words to map integers back to words\n",
        "def word_for_id(integer, tokenizer):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- integer = integer from word_for_id dictionary\n",
        "\tOutputs\t\t\t- word = corresponding word from word_for_id dictionary\n",
        "\t\"\"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVv_Kz5hejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- model = Tensorflow model object after training\n",
        "\t\t\t\t\t\t\t- tokenizer = output from create_tokenizer()\n",
        "\t\t\t\t\t\t\t- photo = output from create_features()\n",
        "\t\t\t\t\t\t\t- max_length = output from max_length()\n",
        "\tOutputs\t\t\t- in_text = predicted description for image\n",
        "\t\"\"\"\n",
        "\t# seed generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over whole length of sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlytc6SphqTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- model = Tensorflow model object after training\n",
        "\t\t\t\t\t\t\t- tokenizer = output from create_tokenizer()\n",
        "\t\t\t\t\t\t\t- photo = output from create_features()\n",
        "\t\t\t\t\t\t\t- max_length = output from max_length()\n",
        "\tOutputs\t\t\t- none, prints results to screen\n",
        "\t\"\"\"\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq2aAZmEOiMY",
        "colab_type": "text"
      },
      "source": [
        "### Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElfjpCj_A4Xa",
        "colab_type": "code",
        "outputId": "098b3c46-5a2d-403b-92bd-39ecfcc9cf55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Generate BLEU scores for validation\n",
        "tic = time.perf_counter()\n",
        "print(\"Validation set:\")\n",
        "evaluate_model(best_model, val_descriptions, val_features, tokenizer, max_length)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model evaluated in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.305754\n",
            "BLEU-2: 0.115258\n",
            "BLEU-3: 0.068763\n",
            "BLEU-4: 0.021409\n",
            "Model evaluated in: 4.90 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sp1oCDxOm_b",
        "colab_type": "text"
      },
      "source": [
        "### Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osXkWEKtDv45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "df1fce0b-b1d3-40f1-e552-118427ebb812"
      },
      "source": [
        "# Generate BLEU scores for testing set\n",
        "tic = time.perf_counter()\n",
        "print(\"Test set:\")\n",
        "evaluate_model(best_model, test_descriptions, test_features, tokenizer, max_length)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model evaluated in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set:\n",
            "BLEU-1: 0.307706\n",
            "BLEU-2: 0.115061\n",
            "BLEU-3: 0.070004\n",
            "BLEU-4: 0.024299\n",
            "Model evaluated in: 7.24 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-0h2J03UmcN",
        "colab_type": "text"
      },
      "source": [
        "# Save files for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m9nRHRqPjQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of data required for evaluation\n",
        "base_model_data = {}\n",
        "base_model_data['test_descriptions'] = test_descriptions\n",
        "base_model_data['test_features'] = test_features\n",
        "base_model_data['val_descriptions'] = val_descriptions\n",
        "base_model_data['val_features'] = val_features\n",
        "base_model_data['tokenizer'] = tokenizer\n",
        "base_model_data['max_length'] = max_length\n",
        "\n",
        "# Save to pickle file for submission\n",
        "dump(base_model_data, open('base_model_data.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvJ80nP_LeVy",
        "colab_type": "text"
      },
      "source": [
        "# SUBMISSION: BASE MODEL AND RESULTS\n",
        "Run cell to print out BLEU scores for validation and testing dataset\n",
        "\n",
        "Requires `base_model_data.pkl` and `base_model.h5` to be located in working directory\n",
        "\n",
        "May take 10-15 minutes to run. Output of BLEU scores for validation set and for testing set will be printed to screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ASUd4nyR5Gf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "0c68a8a9-508f-4ba2-d71b-ee8912430ac4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)\n",
        "\n",
        "# Reverse vectorisation of words to map integers back to words\n",
        "def word_for_id(integer, tokenizer):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- integer = integer from word_for_id dictionary\n",
        "\tOutputs\t\t\t- word = corresponding word from word_for_id dictionary\n",
        "\t\"\"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# Generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- model = Tensorflow model object after training\n",
        "\t\t\t\t\t\t\t- tokenizer = output from create_tokenizer()\n",
        "\t\t\t\t\t\t\t- photo = output from create_features()\n",
        "\t\t\t\t\t\t\t- max_length = output from max_length()\n",
        "\tOutputs\t\t\t- in_text = predicted description for image\n",
        "\t\"\"\"\n",
        "\t# seed generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over whole length of sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text\n",
        "\n",
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- model = Tensorflow model object after training\n",
        "\t\t\t\t\t\t\t- tokenizer = output from create_tokenizer()\n",
        "\t\t\t\t\t\t\t- photo = output from create_features()\n",
        "\t\t\t\t\t\t\t- max_length = output from max_length()\n",
        "\tOutputs\t\t\t- none, prints results to screen\n",
        "\t\"\"\"\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        " \n",
        " # Load base_model_data.pkl\n",
        "data = pickle.load(open('base_model_data.pkl', 'rb'))\n",
        " \n",
        " # Load base_model.h5\n",
        "base_model = tf.keras.models.load_model('base_model.h5')\n",
        "\n",
        "# Get validation BLEU scores\n",
        "print('Validation set:')\n",
        "evaluate_model(base_model, data['val_descriptions'], data['val_features'], \\\n",
        "               data['tokenizer'], data['max_length'])\n",
        "# Get test BLEU scores\n",
        "print('Test set:')\n",
        "evaluate_model(base_model, data['test_descriptions'], data['test_features'], \\\n",
        "               data['tokenizer'], data['max_length'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3104b7d141cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m  \u001b[0;31m# Load base_model.h5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Get validation BLEU scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: base_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    }
  ]
}