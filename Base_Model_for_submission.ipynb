{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CocoTheAussieCat/dl_at3/blob/master/Base_Model_for_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QK7eXHIsc6t",
        "colab_type": "text"
      },
      "source": [
        "# Mount Google Drive Data Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CBM_yUOsezf",
        "colab_type": "code",
        "outputId": "b5f2c4ea-d272-40f9-9435-05ca6ba629e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0g-N2134K2x",
        "colab_type": "text"
      },
      "source": [
        "## Set directory\n",
        "This should be the base directory containing all the Flickr8k files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzzNZH5tvKB",
        "colab_type": "code",
        "outputId": "f0393a44-1b73-46fa-8570-a56c69822d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Direct the workbook into the project folder\n",
        "\n",
        "%cd drive/Shared\\ drives/DL_AT3/Cloud_Folder"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Cloud_Folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X0FuZOkWFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "dataset_dir = os.getcwd() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE3_tIgRkfeL",
        "colab_type": "code",
        "outputId": "0896281d-7ff9-45e7-e795-5fc5faf24f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the directory\n",
        "dataset_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/Shared drives/DL_AT3/Cloud_Folder'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5a1biZwZGqt",
        "colab_type": "code",
        "outputId": "ecf263d6-21b1-4312-803a-ac0b6de98e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/Shared drives/DL_AT3/Cloud_Folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsGMBdWc0SVR",
        "colab_type": "code",
        "outputId": "8672fb9e-368a-4343-8ed2-d152a727fc67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Check the files inside the directory\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints\t\t    Flicker8k_Dataset\t      Flickr8k.token.txt\n",
            "Colab_Model.ipynb\t    Flickr8k_Dataset.zip      Flickr_8k.trainImages.txt\n",
            "CrowdFlowerAnnotations.txt  Flickr_8k.devImages.txt   __MACOSX\n",
            "descriptions.txt\t    Flickr8k.lemma.token.txt  model.png\n",
            "ExpertAnnotations.txt\t    Flickr_8k.testImages.txt  readme.txt\n",
            "features.pkl\t\t    Flickr8k_text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C32A5lyYpJZP",
        "colab_type": "text"
      },
      "source": [
        "## Unzip files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo61q99biV6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Image Dataset\n",
        "#!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUrNOy-UAGT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the Text Dataset\n",
        "#!unzip Flickr8k_text.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cA9BBF43mUj",
        "colab_type": "text"
      },
      "source": [
        "## Set directories\n",
        "Relative paths based on `dataset_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV41L_nfkYWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the dataset directory and relative directories\n",
        "image_dir = dataset_dir + '/Flicker8k_Dataset'\n",
        "caption_dir = dataset_dir + '/Flickr8k.token.txt'\n",
        "train_dir = dataset_dir + '/Flickr_8k.trainImages.txt'\n",
        "test_dir = dataset_dir + '/Flickr_8k.testImages.txt'\n",
        "val_dir = dataset_dir + '/Flickr_8k.devImages.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv7XZ57Sj4TC",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxQY7v5tkDqA",
        "colab_type": "text"
      },
      "source": [
        "Import libraries, set seeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kstP5OvJ4kL5",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQzW9k4cj0oF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import argmax\n",
        "import array as arr\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, LSTM, Embedding, Dropout, Add\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\t\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zb-WQiY3ukX",
        "colab_type": "text"
      },
      "source": [
        "## Set seeds\n",
        "Set seeds for reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NsXujecUtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seeds for numpy and tensorflow\n",
        "tf.random.set_seed(12)\n",
        "np.random.seed(12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBnb6t25n7dn",
        "colab_type": "text"
      },
      "source": [
        "# Image pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCw-CzHVn_kC",
        "colab_type": "text"
      },
      "source": [
        "Extract image features\n",
        "\n",
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Used VGG16 pre-trained model to extract image features by:\n",
        "\n",
        "* Loading VGG16 pre-trained model.\n",
        "\n",
        "* Removing top layer (because this layer is used for classification, which is not what is required)\n",
        "\n",
        "* Extract features from each image by using predict function of VGG16 model.\n",
        "\n",
        "* Create image_id by extracting the characters before .jpg in the file name.\n",
        "\n",
        "* Store these features as vector of length 4096 in dictionary with image_id as key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQPOtkRt5MoH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgiCs_bp5DJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # load the model\n",
        "# model = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
        " \n",
        "# model_new = tf.keras.Sequential([\n",
        "#   model,\n",
        "#   Flatten(),\n",
        "# Dense(4096)])\n",
        "# # remove top layer from model\n",
        "# # model.layers.pop()\n",
        "# # model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "# # print summary\n",
        "# print(model_new.summary())\n",
        "# # extract features from each photo\n",
        "# features = dict() # create empty dictionary to store features in\n",
        "# for name in os.listdir(directory):\n",
        "#     # load an image from file\n",
        "#     filename = directory + '/' + name\n",
        "#     image = load_img(filename, target_size=(224, 224))\n",
        "#     # convert the image pixels to a numpy array\n",
        "#     image = img_to_array(image)\n",
        "#     # reshape data for model\n",
        "#     image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "#     # prepare image for VGG model\n",
        "#     image = preprocess_input(image)\n",
        "#     # get features\n",
        "#     feature = model_new.predict(image, verbose=0)\n",
        "#     # get image id\n",
        "#     image_id = name.split('.')[0]\n",
        "#     # store feature in dictionary using image_id as key\n",
        "#     features[image_id] = feature\n",
        "#     print('>%s' % name)\n",
        "# return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKsiIStoK3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ONLY RUN IF YOU DON'T HAVE features.pkl IN YOUR ENVIRONMENT\n",
        "### TAKES >  1HOUR TO RUN\n",
        "# Extract features from all images\n",
        "#features = extract_features(image_dir)\n",
        "#print('Extracted Features: %d' % len(features))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJ_UxHooeAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save feature as pickle file\n",
        "#dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDrZQgR9oYpZ",
        "colab_type": "text"
      },
      "source": [
        "# Caption pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0HSuKMtzEpp",
        "colab_type": "text"
      },
      "source": [
        "Code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/\n",
        "\n",
        "Get cleaned caption for each image by:\n",
        "\n",
        "* Loading captions from text file.\n",
        "\n",
        "* Creating dictionary of captions using image_id as key.\n",
        "\n",
        "* Clean all captions by removing digits, single letter words (eg: a), punctuation and converting to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_qouDIcpC67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load and read image description file\n",
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tReads all captions from txt file as single string\n",
        "\tInputs\t\t- filename = filename of .txt with image captions\n",
        "\tOutputs\t\t- text = string\n",
        "\t\"\"\"\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUlitqApC69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\t\"\"\"\n",
        "  Inputs      - doc = string, output from load_doc()\n",
        "  Outputs     - mapping = dictionary-list of image_id and captions \n",
        "  \"\"\"\n",
        "\tcaption_dict = {}\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in caption_dict:\n",
        "\t\t\tcaption_dict[image_id] = list()\n",
        "\t\t\t# store description\n",
        "\t\t\tcaption_dict [image_id].append(image_desc)\n",
        "\treturn caption_dict "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_b_6DE3pC7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t\"\"\"\n",
        "  Inputs      - descriptions = raw descriptions\n",
        "  Outputs     - descriptions = cleaned descriptions\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxsvkrzxpC7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert descriptions into vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t\"\"\"\"\n",
        "\tInputs\t\t- descriptions = cleaned descriptions\n",
        "\tOutputs\t\t- all_desc = set containing only unique words from descriptions\n",
        "\n",
        "\t\"\"\"\"\n",
        "\t# build list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXp40MuApC7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save descriptions to file, one image_id and description per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t- descriptions = cleaned descriptions\n",
        "\t\t\t\t\t\t- filename = string, name to saves file to\n",
        "\tOutputs\t\t- saved file\n",
        "\t\"\"\"\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDhFWhjm6tOK",
        "colab_type": "text"
      },
      "source": [
        "## Create descriptions, save to .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYzk-jmpC7G",
        "colab_type": "code",
        "outputId": "467f7f74-5fbd-4d4f-b618-f6999456238f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load descriptions from tokenised text file\n",
        "doc = load_doc(caption_dir)\n",
        "\n",
        "# Create dictionary of image_id and descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "# Clean descriptions by stripping digits, punctuation, single letter words and converting to lowercase\n",
        "clean_descriptions(descriptions)\n",
        "\n",
        "# Create vocab from descriptions and get vocab length\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "# Save descriptions to file, one image_id and description per line\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 4473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7FToIspC7J",
        "colab_type": "text"
      },
      "source": [
        "# Load data for modelling\n",
        "Load pre-processed image features and descriptions in correct format to input into `model.fit()`.\n",
        "\n",
        "Note that `features.pkl` and `descriptions.txt` must be in the working directory folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6F1vMYpC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t- filename = .txt file containing cleaned descriptions\n",
        "\tOutputs\t\t- text\n",
        "\t\"\"\"\n",
        "\t# open file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyGLW9mZpC7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t- filename = .txt file containing cleaned descriptions\n",
        "\tOutputs\t\t- dataset = descriptions with image ids as set\n",
        "\t\"\"\"\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-FH_B4UpC7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t- filename = .txt file containing cleaned descriptions\n",
        "\tOutputs\t\t- dataset = descriptions dictionary with image id as key, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tdescriptions parsed with startseq and endseq\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tsuitable for input to model.fit()\n",
        "\t\"\"\"\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wMH5gzMpC7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t- filename = .pkl file containing feature vectors for images\n",
        "\t\t\t\t\t\t- dataset = string, train, val or test\n",
        "\tOutputs\t\t- features = image features\n",
        "\t\"\"\"\n",
        "\t# load all features\n",
        "\tall_features = pickle.load(open(filename, 'rb'))\n",
        "\t# filter features\n",
        "\tfeatures = {k: all_features[k] for k in dataset}\n",
        "\treturn features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrtTPd0IpC7S",
        "colab_type": "text"
      },
      "source": [
        "## Tokenise descriptions\n",
        "Map unique words to integers using tf.keras tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4QHBL5opC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert dictionary of clean descriptions to list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t- descriptions = dictionary of descriptions\n",
        "\tOutputs\t\t- all_desc = list of descriptions\n",
        "\t\"\"\"\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Bh83aHpC7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\t\"\"\"\n",
        "\tInputs \t\t\t- list of descriptions\n",
        "\tOutputs\t\t\t- tokenizer = tokens as tokenizer object\n",
        "\t\"\"\"\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv8ETrBVpC7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to calculate length of description with most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0pKaW9Sn2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "\t\"\"\"\n",
        "\tInputs\t\t\t- tokenizer = output from create_tokenizer()\n",
        "\t\t\t\t\t\t\t- max_length = output from max_length()\n",
        "\t\t\t\t\t\t\t- descriptions = output from load_clean_descriptions()\n",
        "\t\t\t\t\t\t\t- photos = output from load_photo_features()\n",
        "\t\t\t\t\t\t\t- vocab_size = integer\n",
        "\tOutputs\t\t\t- X1 = numpy array of image features\n",
        "\t\t\t\t\t\t\t- X2 = numpy array of description sequences\n",
        "\t\t\t\t\t\t\t- y = numpy array of next word in description sequences\n",
        "\t\"\"\"\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each image identifier\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# walk through each description for the image\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\t# encode the sequence\n",
        "\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t\t# split one sequence into multiple X,y pairs\n",
        "\t\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t\t# split into input and output pair\n",
        "\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t\t# pad input sequence\n",
        "\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t\t# encode output sequence\n",
        "\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t\t# store\n",
        "\t\t\t\tX1.append(photos[key][0])\n",
        "\t\t\t\tX2.append(in_seq)\n",
        "\t\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDMyp13LpC7f",
        "colab_type": "text"
      },
      "source": [
        "## Load train and validation data for modelling\n",
        "Images loaded as numpy arrays, descriptions tokenised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zk8KGNhAvan",
        "colab_type": "code",
        "outputId": "3b0d946e-6102-42c5-e94c-76a942398278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Load training set\n",
        "train = load_set(train_dir)\n",
        "print('Dataset: %d' % len(train))\n",
        "\n",
        "# Load training set descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train = %d' % len(train_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "train_features = load_photo_features('features.pkl', train)\n",
        "print('Photos: train = %d' % len(train_features))\n",
        "\n",
        "# Prepare sequences of descriptions for train, test and validation sets\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# Determine max sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train = 6000\n",
            "Photos: train = 6000\n",
            "Vocabulary Size: 3848\n",
            "Description Length: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSEqSUTEA5rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKQ-7sGQpC7j",
        "colab_type": "code",
        "outputId": "4f05533b-9f93-44af-b7f6-92814010cee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Load validation set (using devImages)\n",
        "val = load_set(val_dir)\n",
        "print('Dataset: %d' % len(val))\n",
        "\n",
        "# Load training set descriptions\n",
        "val_descriptions = load_clean_descriptions('descriptions.txt', val)\n",
        "print('Descriptions: val = %d' % len(val_descriptions))\n",
        "\n",
        "# Extract training set image features from features.pkl\n",
        "val_features = load_photo_features('features.pkl', val)\n",
        "print('Photos: val = %d' % len(val_features))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: val = 1000\n",
            "Photos: val = 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKMvzjme4upR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create data for modelling\n",
        "X1val, X2val, yval = create_sequences(tokenizer, max_length, val_descriptions, val_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt1mIMIopC7n",
        "colab_type": "text"
      },
      "source": [
        "# Define model\n",
        "Based on merge-model described by Tanti et al. in *Where to put the Image in an Image Caption Generator*\n",
        "\n",
        "source: <https://arxiv.org/abs/1703.09137>\n",
        "\n",
        "code source: https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZllMSBpC7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "\t# feature extractor model\n",
        "\tinputs1 = Input(shape=(4096,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\t# sequence model\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\t# decoder model\n",
        "\tdecoder1 = Add()([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\t# summary\n",
        "\tprint(model.summary())\n",
        "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWe-aYjpC7q",
        "colab_type": "text"
      },
      "source": [
        "# Train model\n",
        "Use checkpoint callbacks to save training information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi12TajEpC7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define where to save checkpoints\n",
        "!mkdir -p checkpoints/base\n",
        "check_dir = 'checkpoints/base/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\n",
        "# Monitor validation loss, saving only the best\n",
        "checkpoint = ModelCheckpoint(check_dir, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngMLZQdpC7t",
        "colab_type": "code",
        "outputId": "658ac921-9255-4251-c741-ddab6c77d2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "# Create base model\n",
        "base_model = define_model(vocab_size, max_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 256)      985088      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 30, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 3848)         988936      dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,613,960\n",
            "Trainable params: 3,613,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqfje5knBVYR",
        "colab_type": "code",
        "outputId": "27440284-6875-43b1-c8fb-acdafe676b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fit model\n",
        "tic = time.perf_counter()\n",
        "base_history = base_model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, validation_data=([X1val, X2val], yval), callbacks = [checkpoint, early_stopping, reduce_lr])\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model ran in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.32062, saving model to checkpoints/base/model-ep001-loss5.931-val_loss5.321.h5\n",
            "1916/1916 - 148s - loss: 5.9306 - val_loss: 5.3206 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.32062 to 5.18643, saving model to checkpoints/base/model-ep002-loss5.376-val_loss5.186.h5\n",
            "1916/1916 - 148s - loss: 5.3760 - val_loss: 5.1864 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.18643 to 5.01077, saving model to checkpoints/base/model-ep003-loss5.175-val_loss5.011.h5\n",
            "1916/1916 - 149s - loss: 5.1753 - val_loss: 5.0108 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.01077 to 4.97305, saving model to checkpoints/base/model-ep004-loss5.065-val_loss4.973.h5\n",
            "1916/1916 - 148s - loss: 5.0647 - val_loss: 4.9730 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: val_loss improved from 4.97305 to 4.95634, saving model to checkpoints/base/model-ep005-loss4.975-val_loss4.956.h5\n",
            "1916/1916 - 147s - loss: 4.9752 - val_loss: 4.9563 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: val_loss improved from 4.95634 to 4.93606, saving model to checkpoints/base/model-ep006-loss4.926-val_loss4.936.h5\n",
            "1916/1916 - 148s - loss: 4.9264 - val_loss: 4.9361 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: val_loss improved from 4.93606 to 4.93053, saving model to checkpoints/base/model-ep007-loss4.864-val_loss4.931.h5\n",
            "1916/1916 - 148s - loss: 4.8637 - val_loss: 4.9305 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 4.93053\n",
            "1916/1916 - 152s - loss: 4.8148 - val_loss: 4.9310 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 4.93053\n",
            "1916/1916 - 151s - loss: 4.7893 - val_loss: 4.9769 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 4.93053\n",
            "1916/1916 - 147s - loss: 4.7419 - val_loss: 4.9488 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: val_loss improved from 4.93053 to 4.84460, saving model to checkpoints/base/model-ep011-loss4.719-val_loss4.845.h5\n",
            "1916/1916 - 146s - loss: 4.7193 - val_loss: 4.8446 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: val_loss improved from 4.84460 to 4.65661, saving model to checkpoints/base/model-ep012-loss4.473-val_loss4.657.h5\n",
            "1916/1916 - 147s - loss: 4.4730 - val_loss: 4.6566 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: val_loss improved from 4.65661 to 4.57457, saving model to checkpoints/base/model-ep013-loss4.282-val_loss4.575.h5\n",
            "1916/1916 - 146s - loss: 4.2819 - val_loss: 4.5746 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: val_loss improved from 4.57457 to 4.56728, saving model to checkpoints/base/model-ep014-loss4.157-val_loss4.567.h5\n",
            "1916/1916 - 147s - loss: 4.1565 - val_loss: 4.5673 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 4.56728\n",
            "1916/1916 - 148s - loss: 4.0336 - val_loss: 4.5727 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: val_loss improved from 4.56728 to 4.55430, saving model to checkpoints/base/model-ep016-loss3.955-val_loss4.554.h5\n",
            "1916/1916 - 148s - loss: 3.9554 - val_loss: 4.5543 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 4.55430\n",
            "1916/1916 - 146s - loss: 3.9056 - val_loss: 4.5847 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 4.55430\n",
            "1916/1916 - 145s - loss: 3.7327 - val_loss: 4.6080 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 4.55430\n",
            "1916/1916 - 145s - loss: 3.7334 - val_loss: 4.6778 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 4.55430\n",
            "1916/1916 - 144s - loss: 3.7885 - val_loss: 4.7283 - lr: 0.0010\n",
            "Model ran in: 49.24 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72_faJBlGxSq",
        "colab_type": "code",
        "outputId": "ef4f151b-9882-4aed-b233-b8e5041e38ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check the model has been saved\n",
        "# my_model directory\n",
        "!ls checkpoints/base"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model-ep001-loss3.522-val_loss5.239.h5\tmodel-ep002-loss3.246-val_loss5.207.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kCEX3ethL49",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qk3hHaDhOGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse vectorisation of words to map integers back to words\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohVv_Kz5hejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\t# seed generation process\n",
        "\tin_text = 'startseq'\n",
        "\t# iterate over whole length of sequence\n",
        "\tfor i in range(max_length):\n",
        "\t\t# integer encode input sequence\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# pad input\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\t# predict next word\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\t# convert probability to integer\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\t# map integer to word\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\t# stop if we cannot map the word\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\t# append as input for generating the next word\n",
        "\t\tin_text += ' ' + word\n",
        "\t\t# stop if we predict the end of the sequence\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlytc6SphqTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate model by predicting captions for images\n",
        "# Compare predicted captions to truth captions\n",
        "# Calculate BLEU-1 through BLEU-4 to determine model fit\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8hiqXV_kyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get best model filename from model.fit output\n",
        "base_best_model = 'checkpoints/base/model-ep016-loss3.955-val_loss4.554.h5'\n",
        "\n",
        "# Load best model\n",
        "base_best_model = tf.keras.models.load_model(base_best_model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElfjpCj_A4Xa",
        "colab_type": "code",
        "outputId": "098b3c46-5a2d-403b-92bd-39ecfcc9cf55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "tic = time.perf_counter()\n",
        "evaluate_model(base_best_model, val_descriptions, val_features, tokenizer, max_length)\n",
        "toc = time.perf_counter()\n",
        "run_time = (toc-tic)/60\n",
        "print(f'Model evaluated in: {run_time:0.2f} minutes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.305754\n",
            "BLEU-2: 0.115258\n",
            "BLEU-3: 0.068763\n",
            "BLEU-4: 0.021409\n",
            "Model evaluated in: 4.90 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}